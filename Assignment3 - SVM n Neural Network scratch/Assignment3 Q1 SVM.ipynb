{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3 - Q1 SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XiiYAdNFWJS2pG1HFY2Jik7sP3R0NhqO",
      "authorship_tag": "ABX9TyOLfKcGzeg2HkQxHyDzKB+r"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "6q0xqTlut9lM"
      },
      "outputs": [],
      "source": [
        "#Q1 Implement SVM from scratch\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt                                 #To plot scatter plot\n",
        "from scipy.optimize import minimize, LinearConstraint           #For SVM optimization\n",
        "from scipy.optimize import Bounds, BFGS                         #For SVM optimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.1 Generate and visualize data\n",
        "def gen_data():\n",
        "    np.random.seed(105)\n",
        "    Positive = np.concatenate((np.random.randn(10, 2) * 0.4 + [1.5, -0.5],\n",
        "                             np.random.randn(10, 2) * 0.4 + [-1.5, 0.5]))\n",
        "    Negative = np.random.randn(20, 2) * 0.3 + [0.0, -0.9]\n",
        "\n",
        "    return Positive, Negative"
      ],
      "metadata": {
        "id": "_2E9BcNd82xH"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2 Plot data\n",
        "def plot_data(Positive, Negative):\n",
        "  x_axis_range = np.linspace(-2.5, 2.5, 20)\n",
        "  plt.scatter(Positive[:, 0], Positive[:, 1], color = \"b\")\n",
        "  plt.scatter(Negative[:, 0], Negative[:, 1], color = \"r\")"
      ],
      "metadata": {
        "id": "a3vC1sbIwXa8"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.3 Fit SVM with linear kernel with slack variable\n",
        "def lagrange_dual(alpha, x, y, ZERO):\n",
        "    result = 0\n",
        "    non_zero_alphas = np.where(alpha > ZERO)[0]\n",
        "    for i in non_zero_alphas:\n",
        "        for k in non_zero_alphas:\n",
        "            #result = result + alpha[i]*alpha[k]*y[i]*y[k]*np.dot(x[i, :], x[k, :]) \n",
        "            result = result + alpha[i]*alpha[k]*y[i]*y[k]*linear_kernel(x[i, :], x[k, :])\n",
        "    result = 0.5*result - sum(alpha)     \n",
        "    return result"
      ],
      "metadata": {
        "id": "61Ay0J_TXV4a"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_kernel(x, y, **kwargs):\n",
        "    # insert your code here \n",
        "    # return linear kernel output\n",
        "    return np.dot(x, y)"
      ],
      "metadata": {
        "id": "Hds5AdehBq62"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X, Y, ZERO, C=None, kernel=None, kwargs = {}):\n",
        "  \n",
        "    '''\n",
        "    X: input_data (Nx2) numpy array\n",
        "    Y: binary target variable (N,) numpy array\n",
        "    C: The slack variable\n",
        "    kernel: kernel function (linear or polynomial)\n",
        "    kwargs: dictionary of arguments\n",
        "    '''\n",
        "    # insert your code here\n",
        "    # Apply optimization of dual function (Equation 7.10: Bishop)\n",
        "    # make use of minimize()\n",
        "    # \n",
        "    m, n = X.shape\n",
        "    np.random.seed(1)\n",
        "    \n",
        "    alpha_0 = np.random.rand(m)*C                       # Initialize alphas to random values\n",
        "    linear_constraint = LinearConstraint(Y, [0], [0])   # Define the constraint\n",
        "    bounds_alpha = Bounds(np.zeros(m), np.full(m, C))   # Define the bounds\n",
        "\n",
        "    # Find the optimal value of alpha\n",
        "    result = minimize(lagrange_dual, alpha_0, args = (X, Y, ZERO), method='trust-constr', \n",
        "                      hess=BFGS(), constraints=[linear_constraint],\n",
        "                      bounds=bounds_alpha)\n",
        "    \n",
        "    # The optimized value of alpha lies in result.x\n",
        "    alpha = result.x\n",
        "    return alpha\n"
      ],
      "metadata": {
        "id": "oNTe21GZB_qU"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1.4 Linear Kernel decision boundary visualization\n",
        "\n",
        "def get_support_vectors(X, Y, lambdas, threshold=10e-5):\n",
        "  # insert your code here\n",
        "  # return support vectors, target variable values of the support vectors, lambdas of support vectoss\n",
        "  # remeber only lambdas >=0 are to be kept\n",
        "  s_vecs =  np.array([]); s_vecs_Y =  np.array([]); s_vecs_lambda =  np.array([])\n",
        "\n",
        "  for x, y, lambda_m in zip (X, Y, lambdas):\n",
        "    if lambda_m > threshold:\n",
        "      s_vecs = np.append(s_vecs, x)\n",
        "      s_vecs_Y = np.append(s_vecs_Y, y)\n",
        "      s_vecs_lambda = np.append(s_vecs_lambda, lambda_m)\n",
        "\n",
        "  return np.reshape(s_vecs, (-1, 2)), s_vecs_Y, s_vecs_lambda     #reshape a flattened s_vecs into shape = 10, 2, otherwise it's 20, 1"
      ],
      "metadata": {
        "id": "arKolSDvCgVq"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_point, s_vecs, s_vecs_Y, s_vecs_lambda, C, ZERO, kernel, kwargs):\n",
        "  '''\n",
        "  test_point: test datapoint\n",
        "  s_vecs: Support vectors \n",
        "  s_vecs_Y: target values of the Support vectors\n",
        "  s_vec_lambda: Dual variables corresponding to the support vectors\n",
        "  kernel, kwargs: kernel function (linear or polynomial)\n",
        "  '''\n",
        "  # insert your code here\n",
        "  # Apply support vectors to predict the class of test_point (Equation 7.13: Bishop)\n",
        "\n",
        "  #Calculate w1, w2\n",
        "  number_of_samples = len(s_vecs)\n",
        "  number_of_features = s_vecs.shape[1]\n",
        "\n",
        "  weight = np.zeros(number_of_features)             #Number of features is the number of weights we want to find\n",
        "\n",
        "  for i in range(number_of_samples):\n",
        "    weight = weight + s_vecs_lambda[i] * s_vecs_Y[i] * s_vecs[i, :]\n",
        "\n",
        "  #Calculate w0\n",
        "  constraint = C - ZERO                            #because C cannot equal to lambda, thus we prepare to find lambda < C\n",
        "  support_vendor_count = 0; w0 = 0\n",
        "\n",
        "  for x, y, lambda_m in zip (s_vecs, s_vecs_Y, s_vecs_lambda):\n",
        "    print(\"lambda_m\", lambda_m, \"constraint\", constraint)\n",
        "    if lambda_m > constraint:\n",
        "      w0 = w0 + y - np.dot(x, weight)\n",
        "      support_vendor_count += 1\n",
        "  \n",
        "  print(\"w0\", w0)\n",
        "  print(\"support_vendor_count\", support_vendor_count)\n",
        "  w0 = w0 / support_vendor_count\n",
        "\n",
        "  prediction = np.sum(test_point * weight) + w0\n",
        "  prediction = np.sign(prediction)              #np.sign returns -1, 0, 1, thus manually put 0 into 1 for labelling\n",
        "  if prediction == 0:\n",
        "    prediction = 1\n",
        "  print(\"prediction\", prediction)\n",
        "\n",
        "  return prediction, w0, weight"
      ],
      "metadata": {
        "id": "maJbFuJ3QTDz"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(Positive, Negative,\n",
        "                           ip_new,\n",
        "                           s_vecs, s_vecs_Y, s_vec_lambda,\n",
        "                           w0, weight,\n",
        "                           kernel, kwargs):\n",
        "  '''\n",
        "  Positive, Negative: Example datapoints from both classes\n",
        "  test_point: test data point (2, )\n",
        "  s_vecs: Support vectors\n",
        "  s_vecs_Y: Target variable values for the support vectors\n",
        "  s_vec_lambda: Dual variables corresponding to the support vectors\n",
        "  kernel, kwargs: kernel function (linear or polynomial)\n",
        "  '''\n",
        "  # make use of plt.contour() and plt.scatter()\n",
        "  # plot decision boundary along with margins\n",
        "\n",
        "  #plot hyperplane\n",
        "  x1_coord = np.array(plt.gca().get_xlim())                           #can use linspace or whatever, the important point is to generate corresponding x of y\n",
        "  x2_coord = np.array(plt.gca().get_ylim()) \n",
        "  x1_meshgrid, x2_meshgrid = np.meshgrid(x1_coord, x2_coord)\n",
        "  y_coord = w0 + weight[0] *  x1_meshgrid + weight[1] * x2_meshgrid\n",
        "\n",
        "  print(\"x1_meshgrid\", x1_meshgrid)\n",
        "  print(\"x2_meshgrid\", x2_meshgrid)\n",
        "  print(\"x1_coord\", x1_coord)\n",
        "  print(\"x2_coord\", x2_coord)\n",
        "  print(\"y\", y_coord)\n",
        "  plt.contour(x1_coord, x2_coord, y_coord, levels=[-1, 0, 1], colors=[\"blue\", \"black\", \"red\"])\n",
        "\n",
        "  #plot margin\n",
        "  '''\n",
        "  x2_positive_margin = 1/weight[1] -w0/weight[1] - weight[0]/weight[1] * x1_coord\n",
        "  x2_negative_margin = -1/weight[1] -w0/weight[1] - weight[0]/weight[1] * x1_coord\n",
        "  plt.plot(x1_coord, x2_positive_margin, color='blue')\n",
        "  plt.plot(x1_coord, x2_negative_margin, color='red')\n",
        "\n",
        "  #plot scatter\n",
        "  x_axis_range = np.linspace(-2.5, 2.5, 20)\n",
        "  plt.scatter(Positive[:, 0], Positive[:, 1], color = \"b\")\n",
        "  plt.scatter(Negative[:, 0], Negative[:, 1], color = \"r\")\n",
        "  '''\n",
        "  \n",
        "  \n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "fGMp-_1ASi2f"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test code\n",
        "if __name__ == '__main__':\n",
        "    Positive, Negative = gen_data()\n",
        "    plot_data(Positive, Negative)\n",
        "    \n",
        "    X = np.concatenate((Positive, Negative))\n",
        "    target_data = np.concatenate((np.ones(Positive.shape[0]), -np.ones(Negative.shape[0])))\n",
        "\n",
        "    size = X.shape[0]\n",
        "    shuffle_data = list(range(size))      #range(40) means numbers  0 - 39 are created and lx12isted\n",
        "    random.shuffle(shuffle_data)\n",
        "    X = X[shuffle_data, :]\n",
        "    Y = target_data[shuffle_data]\n",
        "    ZERO = 1e-2\n",
        "    C = 5\n",
        "\n",
        "    lambdas = fit(X, Y, ZERO, C,  kernel=linear_kernel, kwargs={})    # C is slack variable\n",
        "               \n",
        "    # Get the alpha values for the support vectors, as well as their target data.\n",
        "    s_vecs, s_vecs_Y, s_vecs_lambda = get_support_vectors(X, Y, lambdas)\n",
        "    print(f\"{s_vecs.shape[0]} support vectors found.\\\\n\", s_vecs)\n",
        "\n",
        "    print(\"s_vecs_Y\", s_vecs_Y)\n",
        "    print(\"s_vecs_lambda\", s_vecs_lambda)\n",
        "    \n",
        "    # test new data point\n",
        "    ip_new = np.array([-0.6, 0.75])\n",
        "    prediciton, w0, weight = predict(ip_new, s_vecs, s_vecs_Y, s_vecs_lambda, C, ZERO, kernel=linear_kernel, kwargs={})\n",
        "    \n",
        "    plot_decision_boundary(  Positive, Negative,\n",
        "                             ip_new,\n",
        "                             s_vecs,\n",
        "                             s_vecs_Y,\n",
        "                             s_vecs_lambda,\n",
        "                             w0, weight,\n",
        "                             kernel=linear_kernel,\n",
        "                             kwargs={})"
      ],
      "metadata": {
        "id": "9plmh0ozua0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

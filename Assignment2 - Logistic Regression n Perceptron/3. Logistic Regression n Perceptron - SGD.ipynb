{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Logistic Regression n Perceptron - SGD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1oVjd__DS5Qxcxu-LYRfrnmwwQLG84gsQ",
      "authorship_tag": "ABX9TyNWGI/1nGixC8W04iQjq9vQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LfhZR6MmIFwg",
        "outputId": "acc8002a-fb56-4e0d-d431-ea1658d7d75d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCS6140 Assignment 2\\nQuestion 3 - Logistic Regression and Perceptron\\nJun 3 2022\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 523
        }
      ],
      "source": [
        "'''\n",
        "CS6140 Assignment 2\n",
        "Question 3 - Logistic Regression and Perceptron\n",
        "Jun 3 2022\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split    #Library to split training and testing data\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "zHyqC_AGJa6E"
      },
      "execution_count": 524,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "5H0MmzNNJRsj"
      },
      "execution_count": 525,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "    ''' data: input features\n",
        "        labels: output features\n",
        "    '''\n",
        "    dataset = pd.read_csv(data_dir)  \n",
        "    \n",
        "    features_names = dataset.columns.values[:-1]        #Equipvalent to feature_cols = ['feature1', 'feature2', 'feature3', 'feature4']\n",
        "    features_data = dataset[features_names] \n",
        "    labels_name = dataset.columns.values[-1]   \n",
        "    labels_data = dataset[labels_name]\n",
        "\n",
        "    return features_names, features_data, labels_name, labels_data\n",
        "    \n",
        "    #return features_names, labels_name"
      ],
      "metadata": {
        "id": "HhgFkJDKJTWc"
      },
      "execution_count": 526,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 1"
      ],
      "metadata": {
        "id": "yXoPgJiV4c7g"
      },
      "execution_count": 527,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    sig = 1.0/(1.0+np.exp(-z))\n",
        "    sig = np.minimum(sig, 0.9999)  # Set upper bound\n",
        "    sig = np.maximum(sig, 0.0001)  # Set lower bound\n",
        "    return sig"
      ],
      "metadata": {
        "id": "4kWOy-A0SBoR"
      },
      "execution_count": 528,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Implement the loss function for logistic regression\n",
        "\n",
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "   \n",
        "    num_of_samples = len(op)\n",
        "\n",
        "    y_hat = sigmoid(np.dot(ip, params))\n",
        "    #nll = sum((-op * np.log(y_hat)) - ((1- op)*np.log(1-y_hat))) + alpha/2 * np.dot(params.transpose(), params)\n",
        "    nll = sum((-op * np.log(y_hat)) - ((1- op)*np.log(1-y_hat)))  \n",
        "    cost = nll\n",
        "    gradient = np.dot(ip.transpose(), (y_hat - op))  \n",
        "\n",
        "    '''\n",
        "    print(\"y_hat\", y_hat)\n",
        "    print(\"params\", params)\n",
        "    print(\"gradient\", gradient)\n",
        "    print(\"cost\", cost)\n",
        "    print(\"\\n\")\n",
        "    '''\n",
        "    \n",
        "#     print (cost_sum)\n",
        "    return cost, gradient"
      ],
      "metadata": {
        "id": "ihOBcGt-VRxN"
      },
      "execution_count": 529,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 2"
      ],
      "metadata": {
        "id": "7z3hhHjS4hKH"
      },
      "execution_count": 530,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_using_batch_gradient_descent(ip, op, params, alpha, num_iter, batch_size = 1):\n",
        "    \"\"\"\n",
        "    Compute the params for logistic regression using batch gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    max_iter: maximum number of iterations\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\" \n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    pass\n",
        "    \n",
        "    #batchify the data into mini-batches\n",
        "    pass\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    cost_list = []\n",
        "    for i in range(num_iter):\n",
        "        cost, gradient = compute_cost(ip, op, params)\n",
        "        params = params - (alpha * gradient)\n",
        "        cost_list.append(cost)\n",
        "    \n",
        "    return params, cost_list"
      ],
      "metadata": {
        "id": "_MV9qHqY4WDi"
      },
      "execution_count": 531,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 1 : normalized data by z-score\n",
        "def featureNormalization_zscore(X):\n",
        "    \"\"\"\n",
        "    Take in numpy array of X values and return normalize X values,\n",
        "    the mean and standard deviation of each feature\n",
        "    \"\"\"\n",
        "    mean=np.mean(X,axis=0)\n",
        "    std=np.std(X,axis=0)\n",
        "    \n",
        "    X_norm = (X - mean)/std\n",
        "    \n",
        "    return X_norm , mean , std"
      ],
      "metadata": {
        "id": "iEJ6_M4O575S"
      },
      "execution_count": 532,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 2 : normalized data by min max normalization\n",
        "def featureNormalization_minmax(X):\n",
        "    \"\"\"\n",
        "    Take in numpy array of X values and return normalize X values,\n",
        "    the mean and standard deviation of each feature\n",
        "    \"\"\"\n",
        "    mean=np.mean(X,axis=0)\n",
        "    min=np.min(X, axis=0)\n",
        "    max=np.max(X, axis=0)\n",
        "    \n",
        "    X_norm = (X - min)/(max - min)\n",
        "    std = 0\n",
        "    \n",
        "    return X_norm , mean , std"
      ],
      "metadata": {
        "id": "1Mj5o7tztear"
      },
      "execution_count": 533,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression predictor\n",
        "def classifierPredict(theta,X):\n",
        "    \"\"\"\n",
        "    take in numpy array of theta and X and predict the class \n",
        "    \"\"\"\n",
        "    predictions = X.dot(theta)\n",
        "    \n",
        "    '''\n",
        "    print(\"X\", X)\n",
        "    print(\"predictions\", predictions)\n",
        "    '''\n",
        "    return np.where(predictions >= 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "GiP19ZGY0j_y"
      },
      "execution_count": 534,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 3"
      ],
      "metadata": {
        "id": "emmV6Ua0pjzy"
      },
      "execution_count": 535,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "  # constructor \n",
        "  def __init__ (self, num_of_x_features, alpha):\n",
        "    #self.weight = np.random.randn(num_of_x_features) / np.sqrt(num_of_x_features)\n",
        "    #self.W = np.random.randn(num_of_x_features) / np.sqrt(num_of_x_features)\n",
        "    self.W = np.zeros((num_of_features+1))\n",
        "    self.b = None\n",
        "    self.alpha = alpha\n",
        "\n",
        "  # model\n",
        "  def step(self, x):\n",
        "    return 1 if x > 0 else 0\n",
        "\n",
        "  # fitting the model    \n",
        "  def fit(self, X, y, epochs): \n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      #error = 0\n",
        "      for (x, target) in zip(X, y):\n",
        "        y_hat = self.step(np.dot(x, self.W))\n",
        "        if y_hat != target:\n",
        "          error = target - y_hat\n",
        "          self.W += self.alpha * error * x\n",
        "      #print(\"self.W\", self.W)\n",
        "      \n",
        "  # predictor to predict on the data based on weight\n",
        "  def predict(self, X):\n",
        "    return self.step(np.dot(X, self.W))\n"
      ],
      "metadata": {
        "id": "2eWmI2EWpnaw"
      },
      "execution_count": 536,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "    '''return the accuracy scores'''\n",
        "    \n",
        "    return accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "M13cb8KJIugk"
      },
      "execution_count": 537,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 4"
      ],
      "metadata": {
        "id": "SJ8cwD4GfQvz"
      },
      "execution_count": 538,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "#reserve the test data, do not use them for cross-validation!\n",
        "\n",
        "#load data\n",
        "DATA_DIR = \"/content/drive/My Drive/Colab Notebooks/CS6140 Assignment2/default of credit card clients2.csv\"\n",
        "features_names, features_data, labels_name, labels_data = load_data(DATA_DIR)\n",
        "\n",
        "#split data\n",
        "x_train, x_test, y_train, y_test = train_test_split(features_data, labels_data, test_size=0.2, random_state = 42) #x_train datatype <class 'pandas.core.frame.DataFrame'>\n",
        "\n",
        "#setup numpy print environment\n",
        "np.set_printoptions(suppress=True)      #with np set to printoptions, the printout would not be in scientific e to the power of x format"
      ],
      "metadata": {
        "id": "EuLf-likKYtT"
      },
      "execution_count": 539,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by logistic regression - normalized data\n",
        "num_of_features = x_train.shape[1]      #x_train.shape (3984, 25)\n",
        "\n",
        "#normalize data\n",
        "x_train_minmax_normalized, x_train_mean, x_train_std = featureNormalization_minmax(x_train)\n",
        "x_test_minmax_normalized, x_test_mean, x_test_std = featureNormalization_minmax(x_test)\n",
        "\n",
        "#add bias column\n",
        "x_train_minmax_normalized = np.append(np.ones((x_train_minmax_normalized.shape[0],1)), x_train_minmax_normalized, axis=1)       #x_train datatype <class 'numpy.ndarray'>, append bias with all ones at the end of dataset\n",
        "x_test_minmax_normalized = np.append(np.ones((x_test_minmax_normalized.shape[0],1)), x_test_minmax_normalized, axis=1)       #x_train datatype <class 'numpy.ndarray'>, append bias with all ones at the end of dataset\n",
        "\n",
        "#reshape true label for calculation\n",
        "y_train_reshaped = y_train.values.reshape(x_train.shape[0],1)\n",
        "y_test_reshaped = y_test.values.reshape(x_test.shape[0],1)\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 200\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))  #initialize params as 0 by rows as num of x_train columns, and 1 single column\n",
        "\n",
        "#train the model - run batch gradient descent\n",
        "params, cost = logistic_regression_using_batch_gradient_descent(x_train_minmax_normalized, y_train_reshaped, params, alpha, num_iters, batch_size)\n",
        "\n",
        "#test the model by prediction\n",
        "#y_normalized_logistic_prediction = []\n",
        "p = classifierPredict(params, x_test_minmax_normalized)\n",
        "print(\"prediction\",p.transpose() )\n",
        "print(\"y_test\", y_test.to_numpy())\n",
        "\n",
        "#compute accuracy of the model\n",
        "print(evaluate(y_test, p ))\n",
        "\n",
        "'''\n",
        "count = 0\n",
        "print(\"y_test.shape\", y_test.shape[0])\n",
        "print(\"type(y_test)\", type(y_test), \"type(p)\", type(p))\n",
        "y_test_array = y_test.to_numpy()\n",
        "for i in range(y_test.shape[0]):\n",
        "  if y_test_array[i] == p[i]:\n",
        "    count += 1\n",
        "print(\"count\", count)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ih0c9Ux6lF3q",
        "outputId": "4c5e3ea4-74a4-47fd-b397-2728efbd5421"
      },
      "execution_count": 540,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction [[0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0\n",
            "  0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            "  0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            "  0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            "  1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0\n",
            "  0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            "  1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0\n",
            "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            "  1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0]]\n",
            "y_test [1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "0.7740963855421686\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncount = 0\\nprint(\"y_test.shape\", y_test.shape[0])\\nprint(\"type(y_test)\", type(y_test), \"type(p)\", type(p))\\ny_test_array = y_test.to_numpy()\\nfor i in range(y_test.shape[0]):\\n  if y_test_array[i] == p[i]:\\n    count += 1\\nprint(\"count\", count)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 540
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by logistic regression - raw data\n",
        "\n",
        "#add bias column\n",
        "x_test = np.append(np.ones((x_test.shape[0],1)), x_test, axis=1)          #append bias with all ones at the end of dataset\n",
        "x_train = np.append(np.ones((x_train.shape[0],1)), x_train, axis=1)\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 100\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "#train the model - run batch gradient descent\n",
        "params, cost = logistic_regression_using_batch_gradient_descent(x_train, y_train_reshaped, params, alpha, num_iters, batch_size)\n",
        "\n",
        "#test the model by prediction\n",
        "p2 = classifierPredict(params, x_test)\n",
        "print(\"prediction\",p2.transpose() )\n",
        "print(\"y_test\", y_test.to_numpy())\n",
        "\n",
        "#compute accuracy of the model\n",
        "print(evaluate(y_test, p2 ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpxZr2bpxVVx",
        "outputId": "9ea59ee7-d60a-46d8-a4ca-ff9673eb536d"
      },
      "execution_count": 541,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction [[1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            "  1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            "  0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            "  1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            "  0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0\n",
            "  1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1\n",
            "  0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0\n",
            "  0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0\n",
            "  0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1\n",
            "  0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            "  1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1\n",
            "  0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
            "  0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0\n",
            "  0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1\n",
            "  1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            "  1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            "  1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1\n",
            "  0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
            "  1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1\n",
            "  1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            "  1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1\n",
            "  1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0\n",
            "  0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1\n",
            "  1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1\n",
            "  1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            "  0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1\n",
            "  0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0]]\n",
            "y_test [1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "0.43775100401606426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by perceptron regression - raw data\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=0.1\n",
        "num_iters = 20\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "y_prediction = []\n",
        "\n",
        "#train the model \n",
        "perceptron1 = Perceptron(num_of_features, alpha)\n",
        "perceptron1.fit(x_train, y_train, epochs = num_iters)\n",
        "\n",
        "#test the model\n",
        "for (x, target) in zip(x_test, y_test):\n",
        "  pred = perceptron1.predict(x)\n",
        "  y_prediction.append(pred)\n",
        "\t\n",
        "print(\"y_prediction\",  y_prediction)\n",
        "\n",
        "#compute accuracy of the model\n",
        "print(evaluate(y_test, y_prediction ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcRz3cVj6gnl",
        "outputId": "6f1c3202-6f41-4b4d-d3e1-5f7cf6ed7f72"
      },
      "execution_count": 542,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_prediction [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "0.7429718875502008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by perceptron regression - normalized data\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=0.1\n",
        "num_iters = 20\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "y_normalized_prediction = []\n",
        "\n",
        "#train the model \n",
        "perceptron2 = Perceptron(num_of_features, alpha)\n",
        "perceptron2.fit(x_train_minmax_normalized, y_train, epochs = num_iters)\n",
        "\n",
        "#test the model\n",
        "for (x, target) in zip(x_test_minmax_normalized, y_test):\n",
        "  pred = perceptron2.predict(x)\n",
        "  y_normalized_prediction.append(pred)\n",
        "\n",
        "print(\"y_normalized_prediction\",  y_normalized_prediction)\n",
        "\n",
        "#compute accuracy of the model\n",
        "print(evaluate(y_test, y_normalized_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqfLMPgeHcGP",
        "outputId": "f34f3cd8-2ea6-46dd-8c37-679a7b510a11"
      },
      "execution_count": 543,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_normalized_prediction [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "0.75\n"
          ]
        }
      ]
    }
  ]
}

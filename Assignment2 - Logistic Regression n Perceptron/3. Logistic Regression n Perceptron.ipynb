{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Logistic Regression n Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1oVjd__DS5Qxcxu-LYRfrnmwwQLG84gsQ",
      "authorship_tag": "ABX9TyPcfMB7aEfBMXBZ1FN+LadO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1006,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LfhZR6MmIFwg",
        "outputId": "134e8713-26b0-473d-8928-78d18540bb32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCS6140 Assignment 2\\nQuestion 3 - Logistic Regression and Perceptron\\nJun 3 2022\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1006
        }
      ],
      "source": [
        "'''\n",
        "CS6140 Assignment 2\n",
        "Question 3 - Logistic Regression and Perceptron\n",
        "Jun 3 2022\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split    #Library to split training and testing data\n",
        "from sklearn.metrics import classification_report       #Library to compute accuracy, precison and recall"
      ],
      "metadata": {
        "id": "zHyqC_AGJa6E"
      },
      "execution_count": 1007,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "5H0MmzNNJRsj"
      },
      "execution_count": 1008,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "    ''' data: input features\n",
        "        labels: output features\n",
        "    '''\n",
        "    dataset = pd.read_csv(data_dir)  \n",
        "    \n",
        "    features_names = dataset.columns.values[:-1]        #Equipvalent to feature_cols = ['feature1', 'feature2', 'feature3', 'feature4']\n",
        "    features_data = dataset[features_names] \n",
        "    labels_name = dataset.columns.values[-1]   \n",
        "    labels_data = dataset[labels_name]\n",
        "\n",
        "    return features_names, features_data, labels_name, labels_data\n",
        "    \n",
        "    #return features_names, labels_name"
      ],
      "metadata": {
        "id": "HhgFkJDKJTWc"
      },
      "execution_count": 1009,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 1"
      ],
      "metadata": {
        "id": "yXoPgJiV4c7g"
      },
      "execution_count": 1010,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    sig = 1.0/(1.0+np.exp(-z))\n",
        "    sig = np.minimum(sig, 0.9999)  # Set upper bound\n",
        "    sig = np.maximum(sig, 0.0001)  # Set lower bound\n",
        "    return sig"
      ],
      "metadata": {
        "id": "4kWOy-A0SBoR"
      },
      "execution_count": 1011,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Implement the loss function for logistic regression\n",
        "\n",
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "   \n",
        "    num_of_samples = len(op)\n",
        "\n",
        "    y_hat = sigmoid(np.dot(ip, params))\n",
        "    #nll = sum((-op * np.log(y_hat)) - ((1- op)*np.log(1-y_hat))) + alpha/2 * np.dot(params.transpose(), params)\n",
        "    nll = sum((-op * np.log(y_hat)) - ((1- op)*np.log(1-y_hat)))  \n",
        "    cost = nll\n",
        "    gradient = np.dot(ip.transpose(), (y_hat - op))  \n",
        "\n",
        "    '''\n",
        "    print(\"y_hat\", y_hat)\n",
        "    print(\"params\", params)\n",
        "    print(\"gradient\", gradient)\n",
        "    print(\"cost\", cost)\n",
        "    print(\"\\n\")\n",
        "    '''\n",
        "    \n",
        "#     print (cost_sum)\n",
        "    return cost, gradient"
      ],
      "metadata": {
        "id": "ihOBcGt-VRxN"
      },
      "execution_count": 1012,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 2"
      ],
      "metadata": {
        "id": "7z3hhHjS4hKH"
      },
      "execution_count": 1013,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_using_batch_gradient_descent(ip, op, params, alpha, num_iter, batch_size = 1):\n",
        "    \"\"\"\n",
        "    Compute the params for logistic regression using batch gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    max_iter: maximum number of iterations\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\" \n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    pass\n",
        "    \n",
        "    #batchify the data into mini-batches\n",
        "    pass\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    cost_list = []\n",
        "    for i in range(num_iter):\n",
        "        cost, gradient = compute_cost(ip, op, params)\n",
        "        params = params - (alpha * gradient)\n",
        "        cost_list.append(cost)\n",
        "    \n",
        "    return params, cost_list"
      ],
      "metadata": {
        "id": "_MV9qHqY4WDi"
      },
      "execution_count": 1014,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 1 : normalized data by z-score\n",
        "def featureNormalization_zscore(X):\n",
        "    \"\"\"\n",
        "    Take in numpy array of X values and return normalize X values,\n",
        "    the mean and standard deviation of each feature\n",
        "    \"\"\"\n",
        "    mean=np.mean(X,axis=0)\n",
        "    std=np.std(X,axis=0)\n",
        "    \n",
        "    X_norm = (X - mean)/std\n",
        "    \n",
        "    return X_norm , mean , std"
      ],
      "metadata": {
        "id": "iEJ6_M4O575S"
      },
      "execution_count": 1015,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 2 : normalized data by min max normalization\n",
        "def featureNormalization_minmax(X):\n",
        "    \"\"\"\n",
        "    Take in numpy array of X values and return normalize X values,\n",
        "    the mean and standard deviation of each feature\n",
        "    \"\"\"\n",
        "    mean=np.mean(X,axis=0)\n",
        "    min=np.min(X, axis=0)\n",
        "    max=np.max(X, axis=0)\n",
        "    \n",
        "    X_norm = (X - min)/(max - min)\n",
        "    std = 0\n",
        "    \n",
        "    return X_norm , mean , std"
      ],
      "metadata": {
        "id": "1Mj5o7tztear"
      },
      "execution_count": 1016,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression predictor\n",
        "def classifierPredict(theta,X):\n",
        "    \"\"\"\n",
        "    take in numpy array of theta and X and predict the class \n",
        "    \"\"\"\n",
        "    predictions = X.dot(theta)\n",
        "    \n",
        "    '''\n",
        "    print(\"X\", X)\n",
        "    print(\"predictions\", predictions)\n",
        "    '''\n",
        "    return np.where(predictions >= 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "GiP19ZGY0j_y"
      },
      "execution_count": 1017,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 3"
      ],
      "metadata": {
        "id": "emmV6Ua0pjzy"
      },
      "execution_count": 1018,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "  # constructor \n",
        "  def __init__ (self, num_of_x_features, alpha):\n",
        "    #self.weight = np.random.randn(num_of_x_features) / np.sqrt(num_of_x_features)\n",
        "    #self.W = np.random.randn(num_of_x_features) / np.sqrt(num_of_x_features)\n",
        "    self.W = np.zeros((num_of_features+1))\n",
        "    self.b = None\n",
        "    self.alpha = alpha\n",
        "\n",
        "  # model\n",
        "  def step(self, x):\n",
        "    return 1 if x > 0 else 0\n",
        "\n",
        "  # fitting the model\n",
        "  def fit(self, X, y, epochs): \n",
        "    error = 0\n",
        "    for epoch in range(epochs):\n",
        "      for (x, target) in zip(X, y):\n",
        "        y_hat = self.step(np.dot(x, self.W))\n",
        "        if y_hat != target:\n",
        "          error += (target - y_hat) * x\n",
        "      self.W += self.alpha * error\n",
        "          \n",
        "  '''\n",
        "  #SGD\n",
        "  def fit(self, X, y, epochs): \n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      #error = 0\n",
        "      for (x, target) in zip(X, y):\n",
        "        y_hat = self.step(np.dot(x, self.W))\n",
        "\n",
        "        if y_hat != target:\n",
        "          error = target - y_hat\n",
        "          self.W += self.alpha * error * x\n",
        "      #print(\"self.W\", self.W)\n",
        "  '''\n",
        "\n",
        "      \n",
        "  # predictor to predict on the data based on weight\n",
        "  def predict(self, X):\n",
        "    return self.step(np.dot(X, self.W))\n"
      ],
      "metadata": {
        "id": "2eWmI2EWpnaw"
      },
      "execution_count": 1019,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "    '''return the accuracy scores'''\n",
        "    \n",
        "    return accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "M13cb8KJIugk"
      },
      "execution_count": 1020,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 4"
      ],
      "metadata": {
        "id": "SJ8cwD4GfQvz"
      },
      "execution_count": 1021,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "#reserve the test data, do not use them for cross-validation!\n",
        "\n",
        "#load data\n",
        "DATA_DIR = \"/content/drive/My Drive/Colab Notebooks/CS6140 Assignment2/default of credit card clients2.csv\"\n",
        "features_names, features_data, labels_name, labels_data = load_data(DATA_DIR)\n",
        "\n",
        "#split data\n",
        "x_train, x_test, y_train, y_test = train_test_split(features_data, labels_data, test_size=0.2, random_state = 42) #x_train datatype <class 'pandas.core.frame.DataFrame'>\n",
        "\n",
        "#setup numpy print environment\n",
        "np.set_printoptions(suppress=True)      #with np set to printoptions, the printout would not be in scientific e to the power of x format"
      ],
      "metadata": {
        "id": "EuLf-likKYtT"
      },
      "execution_count": 1022,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by logistic regression - normalized data\n",
        "num_of_features = x_train.shape[1]      #x_train.shape (3984, 25)\n",
        "\n",
        "#normalize data\n",
        "x_train_minmax_normalized, x_train_mean, x_train_std = featureNormalization_minmax(x_train)\n",
        "x_test_minmax_normalized, x_test_mean, x_test_std = featureNormalization_minmax(x_test)\n",
        "\n",
        "#add bias column\n",
        "x_train_minmax_normalized = np.append(np.ones((x_train_minmax_normalized.shape[0],1)), x_train_minmax_normalized, axis=1)       #x_train datatype <class 'numpy.ndarray'>, append bias with all ones at the end of dataset\n",
        "x_test_minmax_normalized = np.append(np.ones((x_test_minmax_normalized.shape[0],1)), x_test_minmax_normalized, axis=1)       #x_train datatype <class 'numpy.ndarray'>, append bias with all ones at the end of dataset\n",
        "\n",
        "#reshape true label for calculation\n",
        "y_train_reshaped = y_train.values.reshape(x_train.shape[0],1)\n",
        "y_test_reshaped = y_test.values.reshape(x_test.shape[0],1)\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 200\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))  #initialize params as 0 by rows as num of x_train columns, and 1 single column\n",
        "\n",
        "#train the model - run batch gradient descent\n",
        "params, cost = logistic_regression_using_batch_gradient_descent(x_train_minmax_normalized, y_train_reshaped, params, alpha, num_iters, batch_size)\n",
        "\n",
        "#test the model by prediction\n",
        "#y_normalized_logistic_prediction = []\n",
        "p = classifierPredict(params, x_test_minmax_normalized)\n",
        "print(\"prediction\",p.transpose() )\n",
        "print(\"y_test\", y_test.to_numpy())\n",
        "\n",
        "#compute accuracy of the model\n",
        "print(classification_report(y_test, p))\n",
        "\n",
        "'''\n",
        "count = 0\n",
        "y_test_array = y_test.to_numpy()\n",
        "for i in range(y_test.shape[0]):\n",
        "  if y_test_array[i] == p[i]:\n",
        "    count += 1\n",
        "print(\"count\", count)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ih0c9Ux6lF3q",
        "outputId": "80153d88-c4a1-4632-82d1-d12eaf612f49"
      },
      "execution_count": 1023,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction [[0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0\n",
            "  0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            "  0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            "  0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            "  1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0\n",
            "  0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            "  1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0\n",
            "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            "  1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0]]\n",
            "y_test [1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       771\n",
            "           1       0.50      0.40      0.44       225\n",
            "\n",
            "    accuracy                           0.77       996\n",
            "   macro avg       0.67      0.64      0.65       996\n",
            "weighted avg       0.76      0.77      0.76       996\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncount = 0\\ny_test_array = y_test.to_numpy()\\nfor i in range(y_test.shape[0]):\\n  if y_test_array[i] == p[i]:\\n    count += 1\\nprint(\"count\", count)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1023
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by logistic regression - raw data\n",
        "\n",
        "#add bias column\n",
        "x_test = np.append(np.ones((x_test.shape[0],1)), x_test, axis=1)          #append bias with all ones at the end of dataset\n",
        "x_train = np.append(np.ones((x_train.shape[0],1)), x_train, axis=1)\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 100\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "#train the model - run batch gradient descent\n",
        "params, cost = logistic_regression_using_batch_gradient_descent(x_train, y_train_reshaped, params, alpha, num_iters, batch_size)\n",
        "\n",
        "#test the model by prediction\n",
        "p2 = classifierPredict(params, x_test)\n",
        "print(\"prediction\",p2.transpose() )\n",
        "print(\"y_test\", y_test.to_numpy())\n",
        "\n",
        "#compute accuracy of the model\n",
        "#print(\"Accuracy:\", evaluate(y_test, p2 ))\n",
        "print(classification_report(y_test, p2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpxZr2bpxVVx",
        "outputId": "719490de-fcad-4b6d-8e9e-def9fee6aba6"
      },
      "execution_count": 1024,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction [[1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            "  1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            "  0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            "  1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            "  0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0\n",
            "  1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1\n",
            "  0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0\n",
            "  0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0\n",
            "  0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1\n",
            "  0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            "  1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1\n",
            "  0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
            "  0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0\n",
            "  0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1\n",
            "  1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            "  1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            "  1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1\n",
            "  0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
            "  1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1\n",
            "  1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            "  1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1\n",
            "  1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0\n",
            "  0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1\n",
            "  1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1\n",
            "  1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            "  0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1\n",
            "  0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0]]\n",
            "y_test [1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.37      0.51       771\n",
            "           1       0.23      0.65      0.34       225\n",
            "\n",
            "    accuracy                           0.44       996\n",
            "   macro avg       0.51      0.51      0.43       996\n",
            "weighted avg       0.66      0.44      0.47       996\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by perceptron regression - raw data\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 150\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "y_prediction = []\n",
        "\n",
        "#train the model \n",
        "perceptron1 = Perceptron(num_of_features, alpha)\n",
        "perceptron1.fit(x_train, y_train, epochs = num_iters)\n",
        "\n",
        "#test the model\n",
        "for (x, target) in zip(x_test, y_test):\n",
        "  pred = perceptron1.predict(x)\n",
        "  y_prediction.append(pred)\n",
        "\t\n",
        "print(\"y_prediction\",  y_prediction)\n",
        "\n",
        "#compute accuracy of the model\n",
        "#print(\"Accuracy:\", evaluate(y_test, y_prediction ))\n",
        "print(classification_report(y_test, y_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcRz3cVj6gnl",
        "outputId": "7bee40be-2edb-4615-82c5-01476e8d298a"
      },
      "execution_count": 1025,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_prediction [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.65      0.72       771\n",
            "           1       0.28      0.47      0.36       225\n",
            "\n",
            "    accuracy                           0.61       996\n",
            "   macro avg       0.55      0.56      0.54       996\n",
            "weighted avg       0.69      0.61      0.64       996\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by perceptron regression - normalized data\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 100\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "y_normalized_prediction = []\n",
        "\n",
        "#train the model \n",
        "perceptron2 = Perceptron(num_of_features, alpha)\n",
        "perceptron2.fit(x_train_minmax_normalized, y_train, epochs = num_iters)\n",
        "\n",
        "#test the model\n",
        "for (x, target) in zip(x_test_minmax_normalized, y_test):\n",
        "  pred = perceptron2.predict(x)\n",
        "  y_normalized_prediction.append(pred)\n",
        "\n",
        "print(\"y_normalized_prediction\",  y_normalized_prediction)\n",
        "\n",
        "#compute accuracy of the model\n",
        "#print(\"Accuracy:\", evaluate(y_test, y_normalized_prediction))\n",
        "print(classification_report(y_test, y_normalized_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqfLMPgeHcGP",
        "outputId": "0caedde9-cfa2-4968-ec26-7410cf0947ce"
      },
      "execution_count": 1026,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_normalized_prediction [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       771\n",
            "           1       0.60      0.31      0.41       225\n",
            "\n",
            "    accuracy                           0.80       996\n",
            "   macro avg       0.71      0.63      0.64       996\n",
            "weighted avg       0.77      0.80      0.77       996\n",
            "\n"
          ]
        }
      ]
    }
  ]
}

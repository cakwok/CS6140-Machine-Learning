{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.3 Calculate RME.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Yo9FumW30mmnr9eLivJZCwNQHPPILTtK",
      "authorship_tag": "ABX9TyNb/HOPfa9qdgSYOWvmB9RW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CS6140 Assignment 1\n",
        "Q2.3 Root Mean Square\n",
        "Wing Man, Kwok\n",
        "May 22 2022\n"
      ],
      "metadata": {
        "id": "Rhk3U7hMr_Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "k4XtZ92Lsgss"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "    num_samples = len(ip)\n",
        "    cost_sum = 0.0\n",
        "    for x,y in zip(ip, op):\n",
        "        y_hat = np.dot(params, np.array([1.0, x]))\n",
        "        cost_sum += (y_hat - y) ** 2\n",
        "    \n",
        "    cost = cost_sum / (num_samples)\n",
        "    \n",
        "    return cost"
      ],
      "metadata": {
        "id": "35IVQu2Cu1KC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xdfe-IZPre4-"
      },
      "outputs": [],
      "source": [
        "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using batch gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    max_iter: maximum number of iterations\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\" \n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    iteration = 0\n",
        "    num_samples = len(ip)\n",
        "    cost = np.zeros(max_iter)\n",
        "    params_store = np.zeros([2, max_iter])\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    while iteration < max_iter:\n",
        "        cost[iteration] = compute_cost(ip, op, params)\n",
        "        params_store[:, iteration] = params\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {iteration}')\n",
        "        print(f'cost: {cost[iteration]}')\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "          y_hat = np.dot(params, np.array([1.0, ip[i]]))\n",
        "          gradient = np.array([1.0, ip[i]]) * (op[i] - y_hat)   #np.array instead of purley ip[i], is because the whole ip[i] features have to be considered\n",
        "          params += alpha * gradient/num_samples\n",
        "          \n",
        "        iteration += 1\n",
        "    \n",
        "    return params, cost, params_store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using stochastic gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    num_samples = len(input_var)\n",
        "    cost = np.zeros(num_samples)\n",
        "    params_store = np.zeros([2, num_samples])\n",
        "    \n",
        "    i = 0\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    for x,y in zip(input_var, output_var):\n",
        "        cost[i] = compute_cost(input_var, output_var, params)\n",
        "        params_store[:, i] = params\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {i}')\n",
        "        print(f'cost: {cost[i]}')\n",
        "        \n",
        "        # Apply stochastic gradient descent\n",
        "       \n",
        "        y_hat = np.dot(params, np.array([1.0, x]))\n",
        "        gradient = np.array([1.0, x]) * (y - y_hat)   #np.array instead of purley ip[i], is because the whole ip[i] features have to be considered\n",
        "        params += alpha * gradient/num_samples\n",
        "          \n",
        "        i += 1\n",
        "    \n",
        "    return params, cost, params_store"
      ],
      "metadata": {
        "id": "oEKO1nxnCBnX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "true_slope = 15\n",
        "true_intercept = 2.4\n",
        "input_var = np.arange(0.0,100.0)\n",
        "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
      ],
      "metadata": {
        "id": "whOYdZ1Ls7gM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "# Training the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
        "\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "\n",
        "alpha_batch = 1e-3\n",
        "max_iter = 100\n",
        "params_hat_batch, cost_batch, params_store_batch =\\\n",
        "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5E09bn8sK4a",
        "outputId": "59b9f13c-7099-433b-c28c-965d82a590f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "iteration: 0\n",
            "cost: 13090156.83930492\n",
            "--------------------------\n",
            "iteration: 1\n",
            "cost: 29547.813225415903\n",
            "--------------------------\n",
            "iteration: 2\n",
            "cost: 13017.345671644836\n",
            "--------------------------\n",
            "iteration: 3\n",
            "cost: 12923.887640553728\n",
            "--------------------------\n",
            "iteration: 4\n",
            "cost: 12918.459536897954\n",
            "--------------------------\n",
            "iteration: 5\n",
            "cost: 12915.448427223244\n",
            "--------------------------\n",
            "iteration: 6\n",
            "cost: 12912.518307826856\n",
            "--------------------------\n",
            "iteration: 7\n",
            "cost: 12909.592320224856\n",
            "--------------------------\n",
            "iteration: 8\n",
            "cost: 12906.667917429668\n",
            "--------------------------\n",
            "iteration: 9\n",
            "cost: 12903.745014302223\n",
            "--------------------------\n",
            "iteration: 10\n",
            "cost: 12900.823607281181\n",
            "--------------------------\n",
            "iteration: 11\n",
            "cost: 12897.903695508776\n",
            "--------------------------\n",
            "iteration: 12\n",
            "cost: 12894.985278217257\n",
            "--------------------------\n",
            "iteration: 13\n",
            "cost: 12892.06835464215\n",
            "--------------------------\n",
            "iteration: 14\n",
            "cost: 12889.15292401955\n",
            "--------------------------\n",
            "iteration: 15\n",
            "cost: 12886.238985585922\n",
            "--------------------------\n",
            "iteration: 16\n",
            "cost: 12883.326538578101\n",
            "--------------------------\n",
            "iteration: 17\n",
            "cost: 12880.415582233323\n",
            "--------------------------\n",
            "iteration: 18\n",
            "cost: 12877.506115789245\n",
            "--------------------------\n",
            "iteration: 19\n",
            "cost: 12874.59813848384\n",
            "--------------------------\n",
            "iteration: 20\n",
            "cost: 12871.691649555556\n",
            "--------------------------\n",
            "iteration: 21\n",
            "cost: 12868.786648243185\n",
            "--------------------------\n",
            "iteration: 22\n",
            "cost: 12865.883133785896\n",
            "--------------------------\n",
            "iteration: 23\n",
            "cost: 12862.981105423274\n",
            "--------------------------\n",
            "iteration: 24\n",
            "cost: 12860.080562395304\n",
            "--------------------------\n",
            "iteration: 25\n",
            "cost: 12857.181503942302\n",
            "--------------------------\n",
            "iteration: 26\n",
            "cost: 12854.283929305044\n",
            "--------------------------\n",
            "iteration: 27\n",
            "cost: 12851.387837724638\n",
            "--------------------------\n",
            "iteration: 28\n",
            "cost: 12848.493228442616\n",
            "--------------------------\n",
            "iteration: 29\n",
            "cost: 12845.600100700874\n",
            "--------------------------\n",
            "iteration: 30\n",
            "cost: 12842.708453741707\n",
            "--------------------------\n",
            "iteration: 31\n",
            "cost: 12839.818286807802\n",
            "--------------------------\n",
            "iteration: 32\n",
            "cost: 12836.92959914222\n",
            "--------------------------\n",
            "iteration: 33\n",
            "cost: 12834.04238998843\n",
            "--------------------------\n",
            "iteration: 34\n",
            "cost: 12831.156658590246\n",
            "--------------------------\n",
            "iteration: 35\n",
            "cost: 12828.272404191915\n",
            "--------------------------\n",
            "iteration: 36\n",
            "cost: 12825.389626038053\n",
            "--------------------------\n",
            "iteration: 37\n",
            "cost: 12822.508323373653\n",
            "--------------------------\n",
            "iteration: 38\n",
            "cost: 12819.628495444103\n",
            "--------------------------\n",
            "iteration: 39\n",
            "cost: 12816.750141495191\n",
            "--------------------------\n",
            "iteration: 40\n",
            "cost: 12813.873260773049\n",
            "--------------------------\n",
            "iteration: 41\n",
            "cost: 12810.997852524246\n",
            "--------------------------\n",
            "iteration: 42\n",
            "cost: 12808.12391599568\n",
            "--------------------------\n",
            "iteration: 43\n",
            "cost: 12805.251450434693\n",
            "--------------------------\n",
            "iteration: 44\n",
            "cost: 12802.380455088974\n",
            "--------------------------\n",
            "iteration: 45\n",
            "cost: 12799.510929206614\n",
            "--------------------------\n",
            "iteration: 46\n",
            "cost: 12796.64287203605\n",
            "--------------------------\n",
            "iteration: 47\n",
            "cost: 12793.776282826158\n",
            "--------------------------\n",
            "iteration: 48\n",
            "cost: 12790.911160826194\n",
            "--------------------------\n",
            "iteration: 49\n",
            "cost: 12788.047505285746\n",
            "--------------------------\n",
            "iteration: 50\n",
            "cost: 12785.185315454819\n",
            "--------------------------\n",
            "iteration: 51\n",
            "cost: 12782.324590583801\n",
            "--------------------------\n",
            "iteration: 52\n",
            "cost: 12779.465329923474\n",
            "--------------------------\n",
            "iteration: 53\n",
            "cost: 12776.60753272496\n",
            "--------------------------\n",
            "iteration: 54\n",
            "cost: 12773.751198239861\n",
            "--------------------------\n",
            "iteration: 55\n",
            "cost: 12770.896325720027\n",
            "--------------------------\n",
            "iteration: 56\n",
            "cost: 12768.042914417787\n",
            "--------------------------\n",
            "iteration: 57\n",
            "cost: 12765.190963585821\n",
            "--------------------------\n",
            "iteration: 58\n",
            "cost: 12762.340472477212\n",
            "--------------------------\n",
            "iteration: 59\n",
            "cost: 12759.491440345384\n",
            "--------------------------\n",
            "iteration: 60\n",
            "cost: 12756.643866444161\n",
            "--------------------------\n",
            "iteration: 61\n",
            "cost: 12753.797750027761\n",
            "--------------------------\n",
            "iteration: 62\n",
            "cost: 12750.953090350775\n",
            "--------------------------\n",
            "iteration: 63\n",
            "cost: 12748.109886668175\n",
            "--------------------------\n",
            "iteration: 64\n",
            "cost: 12745.26813823533\n",
            "--------------------------\n",
            "iteration: 65\n",
            "cost: 12742.427844307958\n",
            "--------------------------\n",
            "iteration: 66\n",
            "cost: 12739.589004142175\n",
            "--------------------------\n",
            "iteration: 67\n",
            "cost: 12736.751616994476\n",
            "--------------------------\n",
            "iteration: 68\n",
            "cost: 12733.915682121735\n",
            "--------------------------\n",
            "iteration: 69\n",
            "cost: 12731.081198781212\n",
            "--------------------------\n",
            "iteration: 70\n",
            "cost: 12728.24816623054\n",
            "--------------------------\n",
            "iteration: 71\n",
            "cost: 12725.41658372773\n",
            "--------------------------\n",
            "iteration: 72\n",
            "cost: 12722.586450531175\n",
            "--------------------------\n",
            "iteration: 73\n",
            "cost: 12719.757765899658\n",
            "--------------------------\n",
            "iteration: 74\n",
            "cost: 12716.930529092304\n",
            "--------------------------\n",
            "iteration: 75\n",
            "cost: 12714.104739368686\n",
            "--------------------------\n",
            "iteration: 76\n",
            "cost: 12711.280395988659\n",
            "--------------------------\n",
            "iteration: 77\n",
            "cost: 12708.45749821256\n",
            "--------------------------\n",
            "iteration: 78\n",
            "cost: 12705.636045301031\n",
            "--------------------------\n",
            "iteration: 79\n",
            "cost: 12702.816036515114\n",
            "--------------------------\n",
            "iteration: 80\n",
            "cost: 12699.997471116223\n",
            "--------------------------\n",
            "iteration: 81\n",
            "cost: 12697.180348366182\n",
            "--------------------------\n",
            "iteration: 82\n",
            "cost: 12694.364667527152\n",
            "--------------------------\n",
            "iteration: 83\n",
            "cost: 12691.550427861672\n",
            "--------------------------\n",
            "iteration: 84\n",
            "cost: 12688.7376286327\n",
            "--------------------------\n",
            "iteration: 85\n",
            "cost: 12685.92626910352\n",
            "--------------------------\n",
            "iteration: 86\n",
            "cost: 12683.116348537835\n",
            "--------------------------\n",
            "iteration: 87\n",
            "cost: 12680.307866199695\n",
            "--------------------------\n",
            "iteration: 88\n",
            "cost: 12677.500821353533\n",
            "--------------------------\n",
            "iteration: 89\n",
            "cost: 12674.695213264156\n",
            "--------------------------\n",
            "iteration: 90\n",
            "cost: 12671.891041196768\n",
            "--------------------------\n",
            "iteration: 91\n",
            "cost: 12669.08830441693\n",
            "--------------------------\n",
            "iteration: 92\n",
            "cost: 12666.287002190571\n",
            "--------------------------\n",
            "iteration: 93\n",
            "cost: 12663.487133784018\n",
            "--------------------------\n",
            "iteration: 94\n",
            "cost: 12660.688698463959\n",
            "--------------------------\n",
            "iteration: 95\n",
            "cost: 12657.89169549745\n",
            "--------------------------\n",
            "iteration: 96\n",
            "cost: 12655.096124151927\n",
            "--------------------------\n",
            "iteration: 97\n",
            "cost: 12652.301983695215\n",
            "--------------------------\n",
            "iteration: 98\n",
            "cost: 12649.509273395492\n",
            "--------------------------\n",
            "iteration: 99\n",
            "cost: 12646.717992521346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "alpha = 1e-3\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "params_hat, cost, params_store =\\\n",
        "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfTKsQ16YAbT",
        "outputId": "7cc5b7f3-5979-4175-c930-2a9637fc22fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "iteration: 0\n",
            "cost: 13116276.228511825\n",
            "--------------------------\n",
            "iteration: 1\n",
            "cost: 13116291.997657\n",
            "--------------------------\n",
            "iteration: 2\n",
            "cost: 13117051.595547473\n",
            "--------------------------\n",
            "iteration: 3\n",
            "cost: 13116832.903385978\n",
            "--------------------------\n",
            "iteration: 4\n",
            "cost: 13116191.101187577\n",
            "--------------------------\n",
            "iteration: 5\n",
            "cost: 13111936.739983846\n",
            "--------------------------\n",
            "iteration: 6\n",
            "cost: 13105418.6980512\n",
            "--------------------------\n",
            "iteration: 7\n",
            "cost: 13096297.534913912\n",
            "--------------------------\n",
            "iteration: 8\n",
            "cost: 13088165.85529024\n",
            "--------------------------\n",
            "iteration: 9\n",
            "cost: 13072257.493464552\n",
            "--------------------------\n",
            "iteration: 10\n",
            "cost: 13055154.681366343\n",
            "--------------------------\n",
            "iteration: 11\n",
            "cost: 13038270.416302878\n",
            "--------------------------\n",
            "iteration: 12\n",
            "cost: 13009813.91857285\n",
            "--------------------------\n",
            "iteration: 13\n",
            "cost: 12979698.276700998\n",
            "--------------------------\n",
            "iteration: 14\n",
            "cost: 12942741.462062228\n",
            "--------------------------\n",
            "iteration: 15\n",
            "cost: 12897953.459947485\n",
            "--------------------------\n",
            "iteration: 16\n",
            "cost: 12843129.801694293\n",
            "--------------------------\n",
            "iteration: 17\n",
            "cost: 12782193.722383438\n",
            "--------------------------\n",
            "iteration: 18\n",
            "cost: 12720391.71323364\n",
            "--------------------------\n",
            "iteration: 19\n",
            "cost: 12634573.525956811\n",
            "--------------------------\n",
            "iteration: 20\n",
            "cost: 12554324.253637716\n",
            "--------------------------\n",
            "iteration: 21\n",
            "cost: 12455333.341131868\n",
            "--------------------------\n",
            "iteration: 22\n",
            "cost: 12352631.380274005\n",
            "--------------------------\n",
            "iteration: 23\n",
            "cost: 12248259.700916411\n",
            "--------------------------\n",
            "iteration: 24\n",
            "cost: 12136544.813018143\n",
            "--------------------------\n",
            "iteration: 25\n",
            "cost: 12001486.487347912\n",
            "--------------------------\n",
            "iteration: 26\n",
            "cost: 11868830.70146972\n",
            "--------------------------\n",
            "iteration: 27\n",
            "cost: 11729319.940191055\n",
            "--------------------------\n",
            "iteration: 28\n",
            "cost: 11572730.449805073\n",
            "--------------------------\n",
            "iteration: 29\n",
            "cost: 11387263.689937906\n",
            "--------------------------\n",
            "iteration: 30\n",
            "cost: 11216759.638670888\n",
            "--------------------------\n",
            "iteration: 31\n",
            "cost: 11032531.312450962\n",
            "--------------------------\n",
            "iteration: 32\n",
            "cost: 10842320.248781683\n",
            "--------------------------\n",
            "iteration: 33\n",
            "cost: 10631446.385325281\n",
            "--------------------------\n",
            "iteration: 34\n",
            "cost: 10427852.992206436\n",
            "--------------------------\n",
            "iteration: 35\n",
            "cost: 10180936.205289675\n",
            "--------------------------\n",
            "iteration: 36\n",
            "cost: 9924286.621703062\n",
            "--------------------------\n",
            "iteration: 37\n",
            "cost: 9663637.383733967\n",
            "--------------------------\n",
            "iteration: 38\n",
            "cost: 9396813.811408008\n",
            "--------------------------\n",
            "iteration: 39\n",
            "cost: 9137871.14984631\n",
            "--------------------------\n",
            "iteration: 40\n",
            "cost: 8888199.241845869\n",
            "--------------------------\n",
            "iteration: 41\n",
            "cost: 8611158.81949491\n",
            "--------------------------\n",
            "iteration: 42\n",
            "cost: 8322995.880259998\n",
            "--------------------------\n",
            "iteration: 43\n",
            "cost: 8024753.5854563955\n",
            "--------------------------\n",
            "iteration: 44\n",
            "cost: 7753409.196969952\n",
            "--------------------------\n",
            "iteration: 45\n",
            "cost: 7448626.795877814\n",
            "--------------------------\n",
            "iteration: 46\n",
            "cost: 7159147.400872197\n",
            "--------------------------\n",
            "iteration: 47\n",
            "cost: 6853620.40365175\n",
            "--------------------------\n",
            "iteration: 48\n",
            "cost: 6557970.728007934\n",
            "--------------------------\n",
            "iteration: 49\n",
            "cost: 6249452.549831072\n",
            "--------------------------\n",
            "iteration: 50\n",
            "cost: 5950291.672019669\n",
            "--------------------------\n",
            "iteration: 51\n",
            "cost: 5663567.109764312\n",
            "--------------------------\n",
            "iteration: 52\n",
            "cost: 5394200.10421531\n",
            "--------------------------\n",
            "iteration: 53\n",
            "cost: 5112592.484662764\n",
            "--------------------------\n",
            "iteration: 54\n",
            "cost: 4849236.579585588\n",
            "--------------------------\n",
            "iteration: 55\n",
            "cost: 4570113.077519759\n",
            "--------------------------\n",
            "iteration: 56\n",
            "cost: 4313833.7184282895\n",
            "--------------------------\n",
            "iteration: 57\n",
            "cost: 4052589.86021728\n",
            "--------------------------\n",
            "iteration: 58\n",
            "cost: 3781508.7984059746\n",
            "--------------------------\n",
            "iteration: 59\n",
            "cost: 3549884.33787732\n",
            "--------------------------\n",
            "iteration: 60\n",
            "cost: 3311270.2518319814\n",
            "--------------------------\n",
            "iteration: 61\n",
            "cost: 3067279.3872447233\n",
            "--------------------------\n",
            "iteration: 62\n",
            "cost: 2842070.134312943\n",
            "--------------------------\n",
            "iteration: 63\n",
            "cost: 2622881.913193954\n",
            "--------------------------\n",
            "iteration: 64\n",
            "cost: 2426696.5548740085\n",
            "--------------------------\n",
            "iteration: 65\n",
            "cost: 2241313.9803175963\n",
            "--------------------------\n",
            "iteration: 66\n",
            "cost: 2062003.9931332786\n",
            "--------------------------\n",
            "iteration: 67\n",
            "cost: 1890308.2987703488\n",
            "--------------------------\n",
            "iteration: 68\n",
            "cost: 1716508.7417453614\n",
            "--------------------------\n",
            "iteration: 69\n",
            "cost: 1574629.4373994723\n",
            "--------------------------\n",
            "iteration: 70\n",
            "cost: 1434483.5940266147\n",
            "--------------------------\n",
            "iteration: 71\n",
            "cost: 1292572.3967884975\n",
            "--------------------------\n",
            "iteration: 72\n",
            "cost: 1174898.2080838836\n",
            "--------------------------\n",
            "iteration: 73\n",
            "cost: 1069727.8228814732\n",
            "--------------------------\n",
            "iteration: 74\n",
            "cost: 956579.4644284138\n",
            "--------------------------\n",
            "iteration: 75\n",
            "cost: 858549.662741603\n",
            "--------------------------\n",
            "iteration: 76\n",
            "cost: 754608.5415450868\n",
            "--------------------------\n",
            "iteration: 77\n",
            "cost: 675346.866501186\n",
            "--------------------------\n",
            "iteration: 78\n",
            "cost: 603601.4124130821\n",
            "--------------------------\n",
            "iteration: 79\n",
            "cost: 536342.4366454917\n",
            "--------------------------\n",
            "iteration: 80\n",
            "cost: 468966.0600932387\n",
            "--------------------------\n",
            "iteration: 81\n",
            "cost: 410185.0492477075\n",
            "--------------------------\n",
            "iteration: 82\n",
            "cost: 353215.95812965924\n",
            "--------------------------\n",
            "iteration: 83\n",
            "cost: 304622.1905577448\n",
            "--------------------------\n",
            "iteration: 84\n",
            "cost: 267208.2044086854\n",
            "--------------------------\n",
            "iteration: 85\n",
            "cost: 225645.7801147614\n",
            "--------------------------\n",
            "iteration: 86\n",
            "cost: 195910.7170961916\n",
            "--------------------------\n",
            "iteration: 87\n",
            "cost: 163250.5157122657\n",
            "--------------------------\n",
            "iteration: 88\n",
            "cost: 139715.31488131304\n",
            "--------------------------\n",
            "iteration: 89\n",
            "cost: 116056.67733615845\n",
            "--------------------------\n",
            "iteration: 90\n",
            "cost: 101048.08525461306\n",
            "--------------------------\n",
            "iteration: 91\n",
            "cost: 86022.91575827368\n",
            "--------------------------\n",
            "iteration: 92\n",
            "cost: 71538.92704383162\n",
            "--------------------------\n",
            "iteration: 93\n",
            "cost: 60463.18686304221\n",
            "--------------------------\n",
            "iteration: 94\n",
            "cost: 50668.12949912389\n",
            "--------------------------\n",
            "iteration: 95\n",
            "cost: 41056.346847896435\n",
            "--------------------------\n",
            "iteration: 96\n",
            "cost: 34659.465352002095\n",
            "--------------------------\n",
            "iteration: 97\n",
            "cost: 29477.129343268512\n",
            "--------------------------\n",
            "iteration: 98\n",
            "cost: 24720.321360207057\n",
            "--------------------------\n",
            "iteration: 99\n",
            "cost: 21688.70044605632\n"
          ]
        }
      ]
    }
  ]
}
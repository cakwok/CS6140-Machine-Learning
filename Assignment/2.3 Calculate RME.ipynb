{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.3 Calculate RME.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Yo9FumW30mmnr9eLivJZCwNQHPPILTtK",
      "authorship_tag": "ABX9TyOZSYUlKLe1V5/GJUYGCa6C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cakwok/CS6140-Machine-Learning/blob/main/2_3_Calculate_RME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CS6140 Assignment 1\n",
        "Q2.3 Root Mean Square\n",
        "Wing Man, Kwok\n",
        "May 22 2022\n"
      ],
      "metadata": {
        "id": "Rhk3U7hMr_Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "k4XtZ92Lsgss"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "    num_samples = len(ip)\n",
        "    cost_sum = 0.0\n",
        "    for x,y in zip(ip, op):\n",
        "        y_hat = np.dot(params, np.array([1.0, x]))\n",
        "        cost_sum += (y_hat - y) ** 2\n",
        "    \n",
        "    cost = cost_sum / (num_samples)\n",
        "    \n",
        "    return cost"
      ],
      "metadata": {
        "id": "35IVQu2Cu1KC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Xdfe-IZPre4-"
      },
      "outputs": [],
      "source": [
        "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using batch gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    max_iter: maximum number of iterations\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\" \n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    iteration = 0\n",
        "    num_samples = len(ip)\n",
        "    cost = np.zeros(max_iter)\n",
        "    params_store = np.zeros([2, max_iter])\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    while iteration < max_iter:\n",
        "        cost[iteration] = compute_cost(ip, op, params)\n",
        "        params_store[:, iteration] = params\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {iteration}')\n",
        "        print(f'cost: {cost[iteration]}')\n",
        "        \n",
        "        ip_with_bias = np.c_[ np.ones(80), ip]       #convert x variables with bias column all 1\n",
        "          \n",
        "        #All calculation at a time, so it is batch. it won't arrive at faster computation nor accuracy, but computer resources require more because all data points are processed at a time \n",
        "        y_hat = np.dot(ip_with_bias, params)  \n",
        "        gradient = np.dot(ip_with_bias.transpose(), (op - y_hat))\n",
        "        params += alpha/num_samples * gradient\n",
        "        \n",
        "        iteration += 1\n",
        "\n",
        "    return params, cost, params_store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using stochastic gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    num_samples = len(input_var)\n",
        "    cost = np.zeros(num_samples)\n",
        "    params_store = np.zeros([2, num_samples])\n",
        "    \n",
        "    i = 0\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    for x,y in zip(input_var, output_var):\n",
        "        cost[i] = compute_cost(input_var, output_var, params)\n",
        "        params_store[:, i] = params\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {i}')\n",
        "        print(f'cost: {cost[i]}')\n",
        "        \n",
        "        # Apply stochastic gradient descent (one calculation at a time, so it is stochastic)\n",
        "        y_hat = np.dot(params, np.array([1.0, x]))  #np.array instead of purley ip[i], is because the whole ip[i] features have to be considered\n",
        "        params += alpha/num_samples * (y - y_hat) * np.array([1.0, x]) \n",
        "       \n",
        "          \n",
        "        i += 1\n",
        "    \n",
        "    return params, cost, params_store"
      ],
      "metadata": {
        "id": "oEKO1nxnCBnX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "true_slope = 15\n",
        "true_intercept = 2.4\n",
        "input_var = np.arange(0.0,100.0)\n",
        "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
      ],
      "metadata": {
        "id": "whOYdZ1Ls7gM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "# Training the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
        "\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "\n",
        "alpha_batch = 1e-3\n",
        "max_iter = 100\n",
        "params_hat_batch, cost_batch, params_store_batch =\\\n",
        "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)\n",
        "\n",
        "print(\"cost_batch\", cost_batch)"
      ],
      "metadata": {
        "id": "U5E09bn8sK4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "alpha = 1e-3\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "params_hat, cost, params_store =\\\n",
        "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfTKsQ16YAbT",
        "outputId": "9442b520-db13-45dd-c1e1-fd1d4509f4af"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "iteration: 0\n",
            "cost: 12977447.672945132\n",
            "--------------------------\n",
            "iteration: 1\n",
            "cost: 12977450.868193822\n",
            "--------------------------\n",
            "iteration: 2\n",
            "cost: 12977637.765682368\n",
            "--------------------------\n",
            "iteration: 3\n",
            "cost: 12977146.511749584\n",
            "--------------------------\n",
            "iteration: 4\n",
            "cost: 12977492.116379358\n",
            "--------------------------\n",
            "iteration: 5\n",
            "cost: 12976278.023003688\n",
            "--------------------------\n",
            "iteration: 6\n",
            "cost: 12971934.716870187\n",
            "--------------------------\n",
            "iteration: 7\n",
            "cost: 12967571.52030999\n",
            "--------------------------\n",
            "iteration: 8\n",
            "cost: 12955568.063147712\n",
            "--------------------------\n",
            "iteration: 9\n",
            "cost: 12945761.040908705\n",
            "--------------------------\n",
            "iteration: 10\n",
            "cost: 12928754.545349726\n",
            "--------------------------\n",
            "iteration: 11\n",
            "cost: 12907845.897804067\n",
            "--------------------------\n",
            "iteration: 12\n",
            "cost: 12875821.500944637\n",
            "--------------------------\n",
            "iteration: 13\n",
            "cost: 12843330.955212072\n",
            "--------------------------\n",
            "iteration: 14\n",
            "cost: 12812231.174761383\n",
            "--------------------------\n",
            "iteration: 15\n",
            "cost: 12761492.131261108\n",
            "--------------------------\n",
            "iteration: 16\n",
            "cost: 12702558.189966716\n",
            "--------------------------\n",
            "iteration: 17\n",
            "cost: 12639558.168692445\n",
            "--------------------------\n",
            "iteration: 18\n",
            "cost: 12563233.150003895\n",
            "--------------------------\n",
            "iteration: 19\n",
            "cost: 12491614.802655261\n",
            "--------------------------\n",
            "iteration: 20\n",
            "cost: 12419624.159737475\n",
            "--------------------------\n",
            "iteration: 21\n",
            "cost: 12335101.195218341\n",
            "--------------------------\n",
            "iteration: 22\n",
            "cost: 12234568.21931242\n",
            "--------------------------\n",
            "iteration: 23\n",
            "cost: 12135051.381499624\n",
            "--------------------------\n",
            "iteration: 24\n",
            "cost: 12019655.67090592\n",
            "--------------------------\n",
            "iteration: 25\n",
            "cost: 11902833.103813183\n",
            "--------------------------\n",
            "iteration: 26\n",
            "cost: 11755789.78617654\n",
            "--------------------------\n",
            "iteration: 27\n",
            "cost: 11590068.669945674\n",
            "--------------------------\n",
            "iteration: 28\n",
            "cost: 11427112.60284297\n",
            "--------------------------\n",
            "iteration: 29\n",
            "cost: 11241090.8916788\n",
            "--------------------------\n",
            "iteration: 30\n",
            "cost: 11046015.03028708\n",
            "--------------------------\n",
            "iteration: 31\n",
            "cost: 10847949.59442521\n",
            "--------------------------\n",
            "iteration: 32\n",
            "cost: 10665318.316724991\n",
            "--------------------------\n",
            "iteration: 33\n",
            "cost: 10441984.531154508\n",
            "--------------------------\n",
            "iteration: 34\n",
            "cost: 10212906.623286394\n",
            "--------------------------\n",
            "iteration: 35\n",
            "cost: 9968796.730786018\n",
            "--------------------------\n",
            "iteration: 36\n",
            "cost: 9724911.08619137\n",
            "--------------------------\n",
            "iteration: 37\n",
            "cost: 9463149.506112415\n",
            "--------------------------\n",
            "iteration: 38\n",
            "cost: 9197794.782814903\n",
            "--------------------------\n",
            "iteration: 39\n",
            "cost: 8943227.815451998\n",
            "--------------------------\n",
            "iteration: 40\n",
            "cost: 8686816.861093672\n",
            "--------------------------\n",
            "iteration: 41\n",
            "cost: 8430493.447434241\n",
            "--------------------------\n",
            "iteration: 42\n",
            "cost: 8171547.039195949\n",
            "--------------------------\n",
            "iteration: 43\n",
            "cost: 7910127.704240318\n",
            "--------------------------\n",
            "iteration: 44\n",
            "cost: 7618886.435496721\n",
            "--------------------------\n",
            "iteration: 45\n",
            "cost: 7351286.985504949\n",
            "--------------------------\n",
            "iteration: 46\n",
            "cost: 7048963.403211295\n",
            "--------------------------\n",
            "iteration: 47\n",
            "cost: 6768193.445096727\n",
            "--------------------------\n",
            "iteration: 48\n",
            "cost: 6466317.300790446\n",
            "--------------------------\n",
            "iteration: 49\n",
            "cost: 6196555.967422256\n",
            "--------------------------\n",
            "iteration: 50\n",
            "cost: 5925061.605076503\n",
            "--------------------------\n",
            "iteration: 51\n",
            "cost: 5640612.292473415\n",
            "--------------------------\n",
            "iteration: 52\n",
            "cost: 5359507.721403662\n",
            "--------------------------\n",
            "iteration: 53\n",
            "cost: 5066601.492718462\n",
            "--------------------------\n",
            "iteration: 54\n",
            "cost: 4807943.816357635\n",
            "--------------------------\n",
            "iteration: 55\n",
            "cost: 4516384.335309968\n",
            "--------------------------\n",
            "iteration: 56\n",
            "cost: 4246681.486948616\n",
            "--------------------------\n",
            "iteration: 57\n",
            "cost: 3997500.5315906517\n",
            "--------------------------\n",
            "iteration: 58\n",
            "cost: 3752474.4160327865\n",
            "--------------------------\n",
            "iteration: 59\n",
            "cost: 3511681.9158479227\n",
            "--------------------------\n",
            "iteration: 60\n",
            "cost: 3279233.498227568\n",
            "--------------------------\n",
            "iteration: 61\n",
            "cost: 3030853.3944639433\n",
            "--------------------------\n",
            "iteration: 62\n",
            "cost: 2814719.0082993726\n",
            "--------------------------\n",
            "iteration: 63\n",
            "cost: 2587031.8147081193\n",
            "--------------------------\n",
            "iteration: 64\n",
            "cost: 2398367.141219593\n",
            "--------------------------\n",
            "iteration: 65\n",
            "cost: 2197465.9071122883\n",
            "--------------------------\n",
            "iteration: 66\n",
            "cost: 2018567.052740158\n",
            "--------------------------\n",
            "iteration: 67\n",
            "cost: 1853399.4279803354\n",
            "--------------------------\n",
            "iteration: 68\n",
            "cost: 1702262.7016856275\n",
            "--------------------------\n",
            "iteration: 69\n",
            "cost: 1561076.5260569344\n",
            "--------------------------\n",
            "iteration: 70\n",
            "cost: 1409388.6793066692\n",
            "--------------------------\n",
            "iteration: 71\n",
            "cost: 1272360.202192437\n",
            "--------------------------\n",
            "iteration: 72\n",
            "cost: 1152402.5887514171\n",
            "--------------------------\n",
            "iteration: 73\n",
            "cost: 1042634.1323895566\n",
            "--------------------------\n",
            "iteration: 74\n",
            "cost: 940619.8668748355\n",
            "--------------------------\n",
            "iteration: 75\n",
            "cost: 847406.6732610739\n",
            "--------------------------\n",
            "iteration: 76\n",
            "cost: 752015.8927235054\n",
            "--------------------------\n",
            "iteration: 77\n",
            "cost: 664728.8246848928\n",
            "--------------------------\n",
            "iteration: 78\n",
            "cost: 589641.5635088912\n",
            "--------------------------\n",
            "iteration: 79\n",
            "cost: 516362.63440920494\n",
            "--------------------------\n",
            "iteration: 80\n",
            "cost: 445573.57682534785\n",
            "--------------------------\n",
            "iteration: 81\n",
            "cost: 391481.94851614325\n",
            "--------------------------\n",
            "iteration: 82\n",
            "cost: 337563.0774905801\n",
            "--------------------------\n",
            "iteration: 83\n",
            "cost: 296773.89979172056\n",
            "--------------------------\n",
            "iteration: 84\n",
            "cost: 251047.08371966062\n",
            "--------------------------\n",
            "iteration: 85\n",
            "cost: 215640.38826162086\n",
            "--------------------------\n",
            "iteration: 86\n",
            "cost: 189291.51566554396\n",
            "--------------------------\n",
            "iteration: 87\n",
            "cost: 164287.96307943732\n",
            "--------------------------\n",
            "iteration: 88\n",
            "cost: 143107.1364726048\n",
            "--------------------------\n",
            "iteration: 89\n",
            "cost: 117121.00838187728\n",
            "--------------------------\n",
            "iteration: 90\n",
            "cost: 95288.00285127421\n",
            "--------------------------\n",
            "iteration: 91\n",
            "cost: 83796.95776966738\n",
            "--------------------------\n",
            "iteration: 92\n",
            "cost: 70878.6052833791\n",
            "--------------------------\n",
            "iteration: 93\n",
            "cost: 62236.63573918747\n",
            "--------------------------\n",
            "iteration: 94\n",
            "cost: 53827.247524727565\n",
            "--------------------------\n",
            "iteration: 95\n",
            "cost: 43418.55876718978\n",
            "--------------------------\n",
            "iteration: 96\n",
            "cost: 36615.985092099596\n",
            "--------------------------\n",
            "iteration: 97\n",
            "cost: 33165.766981373585\n",
            "--------------------------\n",
            "iteration: 98\n",
            "cost: 29540.1486252637\n",
            "--------------------------\n",
            "iteration: 99\n",
            "cost: 24574.035828892793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"params_hat_batch\", params_hat_batch)\n",
        "rms_BatchGradientDescent = np.sqrt(np.mean(np.square(params_hat_batch[0] + params_hat_batch[1]*x_test - y_test)))\n",
        "rms_SGD = np.sqrt(np.mean(np.square(params_hat[0] + params_hat[1]*x_test - y_test)))\n",
        "print(f'batch rms:      {rms_BatchGradientDescent}')\n",
        "print(f'stochastic rms: {rms_SGD}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6GUOd7AZDT0",
        "outputId": "78074ebc-fc99-4ad0-f9e8-0cce031f970c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params_hat_batch [22.17721025 16.79345048]\n",
            "batch rms:      125.13283253592064\n",
            "stochastic rms: 153.6058186850665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "plt.figure()\n",
        "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
        "print(\"cost_batch\", cost_batch)\n",
        "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('normalized cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
        "print(f'min cost with SGD: {np.min(cost)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "8OZeQ9TEYzR8",
        "outputId": "bc3c07fd-dcf2-4473-abc4-9c3c3be8ead9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost_batch [1.37222157e+07 1.95683092e+04 1.09947521e+04 1.10303897e+04\n",
            " 1.10294379e+04 1.10272877e+04 1.10251058e+04 1.10229240e+04\n",
            " 1.10207433e+04 1.10185637e+04 1.10163852e+04 1.10142078e+04\n",
            " 1.10120315e+04 1.10098563e+04 1.10076822e+04 1.10055091e+04\n",
            " 1.10033372e+04 1.10011663e+04 1.09989965e+04 1.09968278e+04\n",
            " 1.09946602e+04 1.09924937e+04 1.09903283e+04 1.09881640e+04\n",
            " 1.09860007e+04 1.09838385e+04 1.09816775e+04 1.09795174e+04\n",
            " 1.09773585e+04 1.09752007e+04 1.09730439e+04 1.09708883e+04\n",
            " 1.09687337e+04 1.09665802e+04 1.09644277e+04 1.09622764e+04\n",
            " 1.09601261e+04 1.09579769e+04 1.09558288e+04 1.09536817e+04\n",
            " 1.09515358e+04 1.09493909e+04 1.09472471e+04 1.09451043e+04\n",
            " 1.09429626e+04 1.09408221e+04 1.09386825e+04 1.09365441e+04\n",
            " 1.09344067e+04 1.09322704e+04 1.09301352e+04 1.09280010e+04\n",
            " 1.09258679e+04 1.09237359e+04 1.09216050e+04 1.09194751e+04\n",
            " 1.09173463e+04 1.09152185e+04 1.09130918e+04 1.09109662e+04\n",
            " 1.09088417e+04 1.09067182e+04 1.09045958e+04 1.09024744e+04\n",
            " 1.09003541e+04 1.08982349e+04 1.08961167e+04 1.08939996e+04\n",
            " 1.08918836e+04 1.08897686e+04 1.08876547e+04 1.08855418e+04\n",
            " 1.08834300e+04 1.08813193e+04 1.08792096e+04 1.08771010e+04\n",
            " 1.08749934e+04 1.08728869e+04 1.08707814e+04 1.08686770e+04\n",
            " 1.08665737e+04 1.08644714e+04 1.08623702e+04 1.08602700e+04\n",
            " 1.08581708e+04 1.08560728e+04 1.08539757e+04 1.08518798e+04\n",
            " 1.08497848e+04 1.08476910e+04 1.08455982e+04 1.08435064e+04\n",
            " 1.08414157e+04 1.08393260e+04 1.08372374e+04 1.08351498e+04\n",
            " 1.08330632e+04 1.08309777e+04 1.08288933e+04 1.08268099e+04]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-9ab9c41ed39b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cost_batch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stochastic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'normalized cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cost' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}

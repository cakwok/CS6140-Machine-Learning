{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.2 Stochastic Gradient Descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LaEt_FpWm9FW3BA0HmTivQmVhpMzC-8Q",
      "authorship_tag": "ABX9TyPIyTRdfdbWLN4P5vfermgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cakwok/CS6140-Machine-Learning/blob/main/2_2_Stochastic_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CS6140 Assignment 1\n",
        "Q2.2 Stochastic gradient descent\n",
        "Wing Man, Kwok\n",
        "May 22 2022"
      ],
      "metadata": {
        "id": "qTNsK2k_R3r-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4Jb8drOMA6o9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "    num_samples = len(ip)\n",
        "    cost_sum = 0.0\n",
        "    for x,y in zip(ip, op):\n",
        "        y_hat = np.dot(params, np.array([1.0, x]))\n",
        "        cost_sum += (y_hat - y) ** 2\n",
        "    \n",
        "    cost = cost_sum / (num_samples)\n",
        "    \n",
        "    return cost"
      ],
      "metadata": {
        "id": "POx4h6oEB9mD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using stochastic gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    num_samples = len(input_var)\n",
        "    cost = np.zeros(num_samples)\n",
        "    params_store = np.zeros([2, num_samples])\n",
        "    \n",
        "    i = 0\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    for x,y in zip(input_var, output_var):\n",
        "        cost[i] = compute_cost(input_var, output_var, params)\n",
        "        params_store[:, i] = params\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {i}')\n",
        "        print(f'cost: {cost[i]}')\n",
        "        \n",
        "        # Apply stochastic gradient descent\n",
        "       \n",
        "        y_hat = np.dot(params, np.array([1.0, x]))\n",
        "        gradient = np.array([1.0, x]) * (y - y_hat)   #np.array instead of purley ip[i], is because the whole ip[i] features have to be considered\n",
        "        params += alpha * gradient/num_samples\n",
        "          \n",
        "        i += 1\n",
        "    \n",
        "    return params, cost, params_store"
      ],
      "metadata": {
        "id": "oEKO1nxnCBnX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "true_slope = 15\n",
        "true_intercept = 2.4\n",
        "input_var = np.arange(0.0,100.0)\n",
        "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
      ],
      "metadata": {
        "id": "VnfnAgzGCFHv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)"
      ],
      "metadata": {
        "id": "b3Biwf5dD7SD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "alpha = 1e-3\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "params_hat, cost, params_store =\\\n",
        "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
      ],
      "metadata": {
        "id": "Xvj-ISy6CT3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

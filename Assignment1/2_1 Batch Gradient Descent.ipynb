{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS6140 Assignment1 2.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-lBmLKzLEBB-S-rpiwydQSQpLvIJibWP",
      "authorship_tag": "ABX9TyOB0N3k5DajcVMhS0gw+B26",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cakwok/CS6140-Machine-Learning/blob/main/CS6140_Assignment1_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CS6140 Assignment 1\n",
        "Q2.1 Batch gradient descent\n"
      ],
      "metadata": {
        "id": "Rhk3U7hMr_Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "k4XtZ92Lsgss"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "    num_samples = len(ip)\n",
        "    cost_sum = 0.0\n",
        "    for x,y in zip(ip, op):\n",
        "        y_hat = np.dot(params, np.array([1.0, x]))\n",
        "        cost_sum += (y_hat - y) ** 2\n",
        "    \n",
        "    cost = cost_sum / (num_samples)\n",
        "    \n",
        "    return cost"
      ],
      "metadata": {
        "id": "35IVQu2Cu1KC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Xdfe-IZPre4-"
      },
      "outputs": [],
      "source": [
        "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using batch gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    max_iter: maximum number of iterations\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\" \n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    iteration = 0\n",
        "    num_samples = len(ip)\n",
        "    cost = np.zeros(max_iter)\n",
        "    params_store = np.zeros([2, max_iter])\n",
        "    \n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    while iteration < max_iter:\n",
        "        cost[iteration] = compute_cost(ip, op, params)\n",
        "        params_store[:, iteration] = params\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {iteration}')\n",
        "        print(f'cost: {cost[iteration]}')\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "          y_hat = np.dot(params, np.array([1.0, ip[i]]))\n",
        "          gradient = np.array([1.0, ip[i]]) * (op[i] - y_hat)   #np.array instead of purley ip[i], is because the whole ip[i] features have to be considered\n",
        "          params += alpha * gradient/num_samples\n",
        "          \n",
        "        iteration += 1\n",
        "    \n",
        "    return params, cost, params_store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "true_slope = 15\n",
        "true_intercept = 2.4\n",
        "input_var = np.arange(0.0,100.0)\n",
        "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
      ],
      "metadata": {
        "id": "whOYdZ1Ls7gM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "# Training the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
        "\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "\n",
        "alpha_batch = 1e-3\n",
        "max_iter = 100\n",
        "params_hat_batch, cost_batch, params_store_batch =\\\n",
        "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5E09bn8sK4a",
        "outputId": "6070b8cc-e440-4e56-cca9-696921ee8d43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "iteration: 0\n",
            "cost: 13245118.765578862\n",
            "--------------------------\n",
            "iteration: 1\n",
            "cost: 22673.57147132357\n",
            "--------------------------\n",
            "iteration: 2\n",
            "cost: 13207.456827028469\n",
            "--------------------------\n",
            "iteration: 3\n",
            "cost: 13283.556933109814\n",
            "--------------------------\n",
            "iteration: 4\n",
            "cost: 13283.311000934682\n",
            "--------------------------\n",
            "iteration: 5\n",
            "cost: 13280.390931786289\n",
            "--------------------------\n",
            "iteration: 6\n",
            "cost: 13277.390309918565\n",
            "--------------------------\n",
            "iteration: 7\n",
            "cost: 13274.388684574675\n",
            "--------------------------\n",
            "iteration: 8\n",
            "cost: 13271.388485424215\n",
            "--------------------------\n",
            "iteration: 9\n",
            "cost: 13268.389785960657\n",
            "--------------------------\n",
            "iteration: 10\n",
            "cost: 13265.392587700253\n",
            "--------------------------\n",
            "iteration: 11\n",
            "cost: 13262.396889960795\n",
            "--------------------------\n",
            "iteration: 12\n",
            "cost: 13259.402691993315\n",
            "--------------------------\n",
            "iteration: 13\n",
            "cost: 13256.409993047138\n",
            "--------------------------\n",
            "iteration: 14\n",
            "cost: 13253.418792371918\n",
            "--------------------------\n",
            "iteration: 15\n",
            "cost: 13250.42908921767\n",
            "--------------------------\n",
            "iteration: 16\n",
            "cost: 13247.440882834775\n",
            "--------------------------\n",
            "iteration: 17\n",
            "cost: 13244.454172474047\n",
            "--------------------------\n",
            "iteration: 18\n",
            "cost: 13241.468957386629\n",
            "--------------------------\n",
            "iteration: 19\n",
            "cost: 13238.485236824024\n",
            "--------------------------\n",
            "iteration: 20\n",
            "cost: 13235.503010038117\n",
            "--------------------------\n",
            "iteration: 21\n",
            "cost: 13232.522276281215\n",
            "--------------------------\n",
            "iteration: 22\n",
            "cost: 13229.543034805954\n",
            "--------------------------\n",
            "iteration: 23\n",
            "cost: 13226.56528486534\n",
            "--------------------------\n",
            "iteration: 24\n",
            "cost: 13223.589025712781\n",
            "--------------------------\n",
            "iteration: 25\n",
            "cost: 13220.614256602048\n",
            "--------------------------\n",
            "iteration: 26\n",
            "cost: 13217.640976787277\n",
            "--------------------------\n",
            "iteration: 27\n",
            "cost: 13214.669185522982\n",
            "--------------------------\n",
            "iteration: 28\n",
            "cost: 13211.69888206405\n",
            "--------------------------\n",
            "iteration: 29\n",
            "cost: 13208.730065665743\n",
            "--------------------------\n",
            "iteration: 30\n",
            "cost: 13205.7627355837\n",
            "--------------------------\n",
            "iteration: 31\n",
            "cost: 13202.796891073933\n",
            "--------------------------\n",
            "iteration: 32\n",
            "cost: 13199.832531392798\n",
            "--------------------------\n",
            "iteration: 33\n",
            "cost: 13196.869655797078\n",
            "--------------------------\n",
            "iteration: 34\n",
            "cost: 13193.908263543872\n",
            "--------------------------\n",
            "iteration: 35\n",
            "cost: 13190.948353890679\n",
            "--------------------------\n",
            "iteration: 36\n",
            "cost: 13187.989926095379\n",
            "--------------------------\n",
            "iteration: 37\n",
            "cost: 13185.032979416195\n",
            "--------------------------\n",
            "iteration: 38\n",
            "cost: 13182.07751311174\n",
            "--------------------------\n",
            "iteration: 39\n",
            "cost: 13179.123526440992\n",
            "--------------------------\n",
            "iteration: 40\n",
            "cost: 13176.17101866331\n",
            "--------------------------\n",
            "iteration: 41\n",
            "cost: 13173.219989038396\n",
            "--------------------------\n",
            "iteration: 42\n",
            "cost: 13170.270436826371\n",
            "--------------------------\n",
            "iteration: 43\n",
            "cost: 13167.322361287701\n",
            "--------------------------\n",
            "iteration: 44\n",
            "cost: 13164.375761683186\n",
            "--------------------------\n",
            "iteration: 45\n",
            "cost: 13161.430637274048\n",
            "--------------------------\n",
            "iteration: 46\n",
            "cost: 13158.486987321865\n",
            "--------------------------\n",
            "iteration: 47\n",
            "cost: 13155.544811088566\n",
            "--------------------------\n",
            "iteration: 48\n",
            "cost: 13152.60410783648\n",
            "--------------------------\n",
            "iteration: 49\n",
            "cost: 13149.66487682827\n",
            "--------------------------\n",
            "iteration: 50\n",
            "cost: 13146.72711732699\n",
            "--------------------------\n",
            "iteration: 51\n",
            "cost: 13143.79082859607\n",
            "--------------------------\n",
            "iteration: 52\n",
            "cost: 13140.856009899278\n",
            "--------------------------\n",
            "iteration: 53\n",
            "cost: 13137.922660500806\n",
            "--------------------------\n",
            "iteration: 54\n",
            "cost: 13134.990779665135\n",
            "--------------------------\n",
            "iteration: 55\n",
            "cost: 13132.06036665719\n",
            "--------------------------\n",
            "iteration: 56\n",
            "cost: 13129.131420742211\n",
            "--------------------------\n",
            "iteration: 57\n",
            "cost: 13126.203941185839\n",
            "--------------------------\n",
            "iteration: 58\n",
            "cost: 13123.277927254065\n",
            "--------------------------\n",
            "iteration: 59\n",
            "cost: 13120.353378213253\n",
            "--------------------------\n",
            "iteration: 60\n",
            "cost: 13117.430293330131\n",
            "--------------------------\n",
            "iteration: 61\n",
            "cost: 13114.50867187179\n",
            "--------------------------\n",
            "iteration: 62\n",
            "cost: 13111.588513105715\n",
            "--------------------------\n",
            "iteration: 63\n",
            "cost: 13108.669816299725\n",
            "--------------------------\n",
            "iteration: 64\n",
            "cost: 13105.752580722014\n",
            "--------------------------\n",
            "iteration: 65\n",
            "cost: 13102.836805641138\n",
            "--------------------------\n",
            "iteration: 66\n",
            "cost: 13099.922490326042\n",
            "--------------------------\n",
            "iteration: 67\n",
            "cost: 13097.009634046037\n",
            "--------------------------\n",
            "iteration: 68\n",
            "cost: 13094.098236070748\n",
            "--------------------------\n",
            "iteration: 69\n",
            "cost: 13091.188295670221\n",
            "--------------------------\n",
            "iteration: 70\n",
            "cost: 13088.279812114832\n",
            "--------------------------\n",
            "iteration: 71\n",
            "cost: 13085.372784675372\n",
            "--------------------------\n",
            "iteration: 72\n",
            "cost: 13082.467212622936\n",
            "--------------------------\n",
            "iteration: 73\n",
            "cost: 13079.563095229018\n",
            "--------------------------\n",
            "iteration: 74\n",
            "cost: 13076.660431765464\n",
            "--------------------------\n",
            "iteration: 75\n",
            "cost: 13073.759221504499\n",
            "--------------------------\n",
            "iteration: 76\n",
            "cost: 13070.859463718723\n",
            "--------------------------\n",
            "iteration: 77\n",
            "cost: 13067.961157681044\n",
            "--------------------------\n",
            "iteration: 78\n",
            "cost: 13065.064302664798\n",
            "--------------------------\n",
            "iteration: 79\n",
            "cost: 13062.168897943646\n",
            "--------------------------\n",
            "iteration: 80\n",
            "cost: 13059.274942791595\n",
            "--------------------------\n",
            "iteration: 81\n",
            "cost: 13056.382436483096\n",
            "--------------------------\n",
            "iteration: 82\n",
            "cost: 13053.491378292887\n",
            "--------------------------\n",
            "iteration: 83\n",
            "cost: 13050.601767496093\n",
            "--------------------------\n",
            "iteration: 84\n",
            "cost: 13047.713603368193\n",
            "--------------------------\n",
            "iteration: 85\n",
            "cost: 13044.82688518507\n",
            "--------------------------\n",
            "iteration: 86\n",
            "cost: 13041.941612222898\n",
            "--------------------------\n",
            "iteration: 87\n",
            "cost: 13039.057783758286\n",
            "--------------------------\n",
            "iteration: 88\n",
            "cost: 13036.17539906816\n",
            "--------------------------\n",
            "iteration: 89\n",
            "cost: 13033.294457429809\n",
            "--------------------------\n",
            "iteration: 90\n",
            "cost: 13030.4149581209\n",
            "--------------------------\n",
            "iteration: 91\n",
            "cost: 13027.536900419462\n",
            "--------------------------\n",
            "iteration: 92\n",
            "cost: 13024.660283603882\n",
            "--------------------------\n",
            "iteration: 93\n",
            "cost: 13021.785106952888\n",
            "--------------------------\n",
            "iteration: 94\n",
            "cost: 13018.911369745601\n",
            "--------------------------\n",
            "iteration: 95\n",
            "cost: 13016.039071261484\n",
            "--------------------------\n",
            "iteration: 96\n",
            "cost: 13013.168210780366\n",
            "--------------------------\n",
            "iteration: 97\n",
            "cost: 13010.29878758244\n",
            "--------------------------\n",
            "iteration: 98\n",
            "cost: 13007.43080094825\n",
            "--------------------------\n",
            "iteration: 99\n",
            "cost: 13004.564250158697\n"
          ]
        }
      ]
    }
  ]
}

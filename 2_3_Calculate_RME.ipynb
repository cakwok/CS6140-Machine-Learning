{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.3 Calculate RME.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Yo9FumW30mmnr9eLivJZCwNQHPPILTtK",
      "authorship_tag": "ABX9TyOY/cMsDT3El+g8H0ma9sj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cakwok/CS6140-Machine-Learning/blob/main/2_3_Calculate_RME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CS6140 Assignment 1\n",
        "Q2.3 Root Mean Square\n",
        "Wing Man, Kwok\n",
        "May 22 2022\n"
      ],
      "metadata": {
        "id": "Rhk3U7hMr_Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "k4XtZ92Lsgss"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(ip, op, params):\n",
        "    \"\"\"\n",
        "    Cost function in linear regression where the cost is calculated\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    Returns cost\n",
        "    \"\"\"\n",
        "    num_samples = len(ip)\n",
        "    cost_sum = 0.0\n",
        "    for x,y in zip(ip, op):\n",
        "        y_hat = np.dot(params, np.array([1.0, x]))\n",
        "        cost_sum += (y_hat - y) ** 2\n",
        "    \n",
        "    cost = cost_sum / (num_samples)\n",
        "    \n",
        "    return cost"
      ],
      "metadata": {
        "id": "35IVQu2Cu1KC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xdfe-IZPre4-"
      },
      "outputs": [],
      "source": [
        "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using batch gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    max_iter: maximum number of iterations\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\" \n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    iteration = 0\n",
        "    num_samples = len(ip)\n",
        "    cost = np.zeros(max_iter)\n",
        "    params_store = np.zeros([2, max_iter])\n",
        "    ip_with_bias = np.c_[ np.ones(ip.size), ip]       #convert x variables with bias column all 1\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    while iteration < max_iter:\n",
        "\n",
        "        #All calculation at a time, so it is batch. it won't arrive at faster computation nor accuracy, but computer resources require more because all data points are processed at a time \n",
        "        cost[iteration] = compute_cost(ip, op, params)  \n",
        "        params_store[:, iteration] = params\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {iteration}')\n",
        "        print(f'cost: {cost[iteration]}')\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "          y_hat = np.dot(params, np.array([1.0, ip[i]]))\n",
        "          params += alpha/num_samples * (op[i] - y_hat) * np.array([1.0, ip[i]])\n",
        "\n",
        "        iteration += 1\n",
        "      \n",
        "    print(\"run time\", time.time() - start_time)\n",
        "    \n",
        "    return params, cost, params_store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
        "    \"\"\"\n",
        "    Compute the params for linear regression using stochastic gradient descent\n",
        "    ip: input variables\n",
        "    op: output variables\n",
        "    params: corresponding parameters\n",
        "    alpha: learning rate\n",
        "    Returns parameters, cost, params_store\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize iteration, number of samples, cost and parameter array\n",
        "    num_samples = len(input_var)\n",
        "    cost = np.zeros(num_samples)\n",
        "    params_store = np.zeros([2, num_samples])\n",
        "    \n",
        "    i = 0\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    # Compute the cost and store the params for the corresponding cost\n",
        "    for x,y in zip(input_var, output_var):\n",
        "        cost[i] = compute_cost(input_var, output_var, params)\n",
        "        params_store[:, i] = params\n",
        "        \n",
        "        print('--------------------------')\n",
        "        print(f'iteration: {i}')\n",
        "        print(f'cost: {cost[i]}')\n",
        "        \n",
        "        # Apply stochastic gradient descent (one calculation at a time, so it is stochastic)\n",
        "        y_hat = np.dot(params, np.array([1.0, x]))  #np.array instead of purley ip[i], is because the whole ip[i] features have to be considered\n",
        "        params += alpha/num_samples * (y - y_hat) * np.array([1.0, x]) \n",
        "          \n",
        "        i += 1\n",
        "    \n",
        "    print(\"run time:\", time.time() - start_time)\n",
        "    \n",
        "    return params, cost, params_store"
      ],
      "metadata": {
        "id": "oEKO1nxnCBnX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "true_slope = 15\n",
        "true_intercept = 2.4\n",
        "input_var = np.arange(0.0,100.0)\n",
        "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
      ],
      "metadata": {
        "id": "whOYdZ1Ls7gM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "# Training the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
        "\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "\n",
        "alpha_batch = 1e-3\n",
        "max_iter = 100\n",
        "params_hat_batch, cost_batch, params_store_batch =\\\n",
        "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
      ],
      "metadata": {
        "id": "U5E09bn8sK4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7c7342-95fb-4cb5-d918-2a2aa6c31e3e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "iteration: 0\n",
            "cost: 13043408.353073804\n",
            "--------------------------\n",
            "iteration: 1\n",
            "cost: 24677.85128294303\n",
            "--------------------------\n",
            "iteration: 2\n",
            "cost: 11138.671135226723\n",
            "--------------------------\n",
            "iteration: 3\n",
            "cost: 11145.460259094052\n",
            "--------------------------\n",
            "iteration: 4\n",
            "cost: 11144.782742404164\n",
            "--------------------------\n",
            "iteration: 5\n",
            "cost: 11143.361366392774\n",
            "--------------------------\n",
            "iteration: 6\n",
            "cost: 11141.915546603135\n",
            "--------------------------\n",
            "iteration: 7\n",
            "cost: 11140.469606205152\n",
            "--------------------------\n",
            "iteration: 8\n",
            "cost: 11139.02434944415\n",
            "--------------------------\n",
            "iteration: 9\n",
            "cost: 11137.579802567347\n",
            "--------------------------\n",
            "iteration: 10\n",
            "cost: 11136.135966105188\n",
            "--------------------------\n",
            "iteration: 11\n",
            "cost: 11134.692839738249\n",
            "--------------------------\n",
            "iteration: 12\n",
            "cost: 11133.25042311916\n",
            "--------------------------\n",
            "iteration: 13\n",
            "cost: 11131.808715899797\n",
            "--------------------------\n",
            "iteration: 14\n",
            "cost: 11130.367717732184\n",
            "--------------------------\n",
            "iteration: 15\n",
            "cost: 11128.927428268496\n",
            "--------------------------\n",
            "iteration: 16\n",
            "cost: 11127.487847161103\n",
            "--------------------------\n",
            "iteration: 17\n",
            "cost: 11126.048974062525\n",
            "--------------------------\n",
            "iteration: 18\n",
            "cost: 11124.610808625474\n",
            "--------------------------\n",
            "iteration: 19\n",
            "cost: 11123.173350502791\n",
            "--------------------------\n",
            "iteration: 20\n",
            "cost: 11121.736599347538\n",
            "--------------------------\n",
            "iteration: 21\n",
            "cost: 11120.300554812917\n",
            "--------------------------\n",
            "iteration: 22\n",
            "cost: 11118.865216552318\n",
            "--------------------------\n",
            "iteration: 23\n",
            "cost: 11117.43058421926\n",
            "--------------------------\n",
            "iteration: 24\n",
            "cost: 11115.996657467489\n",
            "--------------------------\n",
            "iteration: 25\n",
            "cost: 11114.563435950886\n",
            "--------------------------\n",
            "iteration: 26\n",
            "cost: 11113.130919323494\n",
            "--------------------------\n",
            "iteration: 27\n",
            "cost: 11111.69910723955\n",
            "--------------------------\n",
            "iteration: 28\n",
            "cost: 11110.267999353453\n",
            "--------------------------\n",
            "iteration: 29\n",
            "cost: 11108.83759531976\n",
            "--------------------------\n",
            "iteration: 30\n",
            "cost: 11107.407894793218\n",
            "--------------------------\n",
            "iteration: 31\n",
            "cost: 11105.978897428715\n",
            "--------------------------\n",
            "iteration: 32\n",
            "cost: 11104.550602881334\n",
            "--------------------------\n",
            "iteration: 33\n",
            "cost: 11103.123010806306\n",
            "--------------------------\n",
            "iteration: 34\n",
            "cost: 11101.69612085907\n",
            "--------------------------\n",
            "iteration: 35\n",
            "cost: 11100.269932695164\n",
            "--------------------------\n",
            "iteration: 36\n",
            "cost: 11098.844445970362\n",
            "--------------------------\n",
            "iteration: 37\n",
            "cost: 11097.41966034058\n",
            "--------------------------\n",
            "iteration: 38\n",
            "cost: 11095.995575461897\n",
            "--------------------------\n",
            "iteration: 39\n",
            "cost: 11094.572190990566\n",
            "--------------------------\n",
            "iteration: 40\n",
            "cost: 11093.149506583017\n",
            "--------------------------\n",
            "iteration: 41\n",
            "cost: 11091.727521895826\n",
            "--------------------------\n",
            "iteration: 42\n",
            "cost: 11090.306236585764\n",
            "--------------------------\n",
            "iteration: 43\n",
            "cost: 11088.88565030976\n",
            "--------------------------\n",
            "iteration: 44\n",
            "cost: 11087.46576272491\n",
            "--------------------------\n",
            "iteration: 45\n",
            "cost: 11086.046573488458\n",
            "--------------------------\n",
            "iteration: 46\n",
            "cost: 11084.628082257854\n",
            "--------------------------\n",
            "iteration: 47\n",
            "cost: 11083.210288690687\n",
            "--------------------------\n",
            "iteration: 48\n",
            "cost: 11081.793192444735\n",
            "--------------------------\n",
            "iteration: 49\n",
            "cost: 11080.376793177918\n",
            "--------------------------\n",
            "iteration: 50\n",
            "cost: 11078.961090548333\n",
            "--------------------------\n",
            "iteration: 51\n",
            "cost: 11077.54608421428\n",
            "--------------------------\n",
            "iteration: 52\n",
            "cost: 11076.131773834164\n",
            "--------------------------\n",
            "iteration: 53\n",
            "cost: 11074.718159066608\n",
            "--------------------------\n",
            "iteration: 54\n",
            "cost: 11073.305239570367\n",
            "--------------------------\n",
            "iteration: 55\n",
            "cost: 11071.893015004396\n",
            "--------------------------\n",
            "iteration: 56\n",
            "cost: 11070.481485027776\n",
            "--------------------------\n",
            "iteration: 57\n",
            "cost: 11069.070649299807\n",
            "--------------------------\n",
            "iteration: 58\n",
            "cost: 11067.660507479923\n",
            "--------------------------\n",
            "iteration: 59\n",
            "cost: 11066.251059227718\n",
            "--------------------------\n",
            "iteration: 60\n",
            "cost: 11064.84230420297\n",
            "--------------------------\n",
            "iteration: 61\n",
            "cost: 11063.434242065621\n",
            "--------------------------\n",
            "iteration: 62\n",
            "cost: 11062.026872475773\n",
            "--------------------------\n",
            "iteration: 63\n",
            "cost: 11060.620195093703\n",
            "--------------------------\n",
            "iteration: 64\n",
            "cost: 11059.214209579852\n",
            "--------------------------\n",
            "iteration: 65\n",
            "cost: 11057.80891559482\n",
            "--------------------------\n",
            "iteration: 66\n",
            "cost: 11056.404312799385\n",
            "--------------------------\n",
            "iteration: 67\n",
            "cost: 11055.000400854475\n",
            "--------------------------\n",
            "iteration: 68\n",
            "cost: 11053.597179421207\n",
            "--------------------------\n",
            "iteration: 69\n",
            "cost: 11052.194648160843\n",
            "--------------------------\n",
            "iteration: 70\n",
            "cost: 11050.792806734818\n",
            "--------------------------\n",
            "iteration: 71\n",
            "cost: 11049.39165480474\n",
            "--------------------------\n",
            "iteration: 72\n",
            "cost: 11047.991192032376\n",
            "--------------------------\n",
            "iteration: 73\n",
            "cost: 11046.59141807965\n",
            "--------------------------\n",
            "iteration: 74\n",
            "cost: 11045.192332608665\n",
            "--------------------------\n",
            "iteration: 75\n",
            "cost: 11043.793935281697\n",
            "--------------------------\n",
            "iteration: 76\n",
            "cost: 11042.396225761155\n",
            "--------------------------\n",
            "iteration: 77\n",
            "cost: 11040.999203709654\n",
            "--------------------------\n",
            "iteration: 78\n",
            "cost: 11039.602868789954\n",
            "--------------------------\n",
            "iteration: 79\n",
            "cost: 11038.207220664966\n",
            "--------------------------\n",
            "iteration: 80\n",
            "cost: 11036.812258997788\n",
            "--------------------------\n",
            "iteration: 81\n",
            "cost: 11035.417983451682\n",
            "--------------------------\n",
            "iteration: 82\n",
            "cost: 11034.024393690066\n",
            "--------------------------\n",
            "iteration: 83\n",
            "cost: 11032.631489376523\n",
            "--------------------------\n",
            "iteration: 84\n",
            "cost: 11031.239270174803\n",
            "--------------------------\n",
            "iteration: 85\n",
            "cost: 11029.847735748821\n",
            "--------------------------\n",
            "iteration: 86\n",
            "cost: 11028.456885762667\n",
            "--------------------------\n",
            "iteration: 87\n",
            "cost: 11027.066719880579\n",
            "--------------------------\n",
            "iteration: 88\n",
            "cost: 11025.677237766962\n",
            "--------------------------\n",
            "iteration: 89\n",
            "cost: 11024.288439086393\n",
            "--------------------------\n",
            "iteration: 90\n",
            "cost: 11022.900323503614\n",
            "--------------------------\n",
            "iteration: 91\n",
            "cost: 11021.512890683527\n",
            "--------------------------\n",
            "iteration: 92\n",
            "cost: 11020.12614029118\n",
            "--------------------------\n",
            "iteration: 93\n",
            "cost: 11018.74007199184\n",
            "--------------------------\n",
            "iteration: 94\n",
            "cost: 11017.354685450862\n",
            "--------------------------\n",
            "iteration: 95\n",
            "cost: 11015.96998033384\n",
            "--------------------------\n",
            "iteration: 96\n",
            "cost: 11014.585956306455\n",
            "--------------------------\n",
            "iteration: 97\n",
            "cost: 11013.202613034633\n",
            "--------------------------\n",
            "iteration: 98\n",
            "cost: 11011.819950184405\n",
            "--------------------------\n",
            "iteration: 99\n",
            "cost: 11010.437967421993\n",
            "run time 0.3732872009277344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "alpha = 1e-3\n",
        "params_0 = np.array([20.0, 80.0])\n",
        "params_hat, cost, params_store =\\\n",
        "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
      ],
      "metadata": {
        "id": "DfTKsQ16YAbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d630b9a-0142-43c5-a203-01520243d5af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "iteration: 0\n",
            "cost: 13078440.379060538\n",
            "--------------------------\n",
            "iteration: 1\n",
            "cost: 13078457.240507754\n",
            "--------------------------\n",
            "iteration: 2\n",
            "cost: 13078626.317507174\n",
            "--------------------------\n",
            "iteration: 3\n",
            "cost: 13079536.532846138\n",
            "--------------------------\n",
            "iteration: 4\n",
            "cost: 13079577.252786893\n",
            "--------------------------\n",
            "iteration: 5\n",
            "cost: 13075526.334292794\n",
            "--------------------------\n",
            "iteration: 6\n",
            "cost: 13069401.655911792\n",
            "--------------------------\n",
            "iteration: 7\n",
            "cost: 13063856.848961623\n",
            "--------------------------\n",
            "iteration: 8\n",
            "cost: 13050186.440672558\n",
            "--------------------------\n",
            "iteration: 9\n",
            "cost: 13033912.287059875\n",
            "--------------------------\n",
            "iteration: 10\n",
            "cost: 13012824.953597067\n",
            "--------------------------\n",
            "iteration: 11\n",
            "cost: 12987985.35242543\n",
            "--------------------------\n",
            "iteration: 12\n",
            "cost: 12967825.236598287\n",
            "--------------------------\n",
            "iteration: 13\n",
            "cost: 12932962.51635226\n",
            "--------------------------\n",
            "iteration: 14\n",
            "cost: 12888915.985800946\n",
            "--------------------------\n",
            "iteration: 15\n",
            "cost: 12850423.67539266\n",
            "--------------------------\n",
            "iteration: 16\n",
            "cost: 12798040.293318374\n",
            "--------------------------\n",
            "iteration: 17\n",
            "cost: 12748366.403161746\n",
            "--------------------------\n",
            "iteration: 18\n",
            "cost: 12673880.61401155\n",
            "--------------------------\n",
            "iteration: 19\n",
            "cost: 12605830.580645822\n",
            "--------------------------\n",
            "iteration: 20\n",
            "cost: 12513761.84593371\n",
            "--------------------------\n",
            "iteration: 21\n",
            "cost: 12426771.367077319\n",
            "--------------------------\n",
            "iteration: 22\n",
            "cost: 12315094.925501948\n",
            "--------------------------\n",
            "iteration: 23\n",
            "cost: 12201111.72375896\n",
            "--------------------------\n",
            "iteration: 24\n",
            "cost: 12071276.470406234\n",
            "--------------------------\n",
            "iteration: 25\n",
            "cost: 11940448.54988564\n",
            "--------------------------\n",
            "iteration: 26\n",
            "cost: 11785855.66802135\n",
            "--------------------------\n",
            "iteration: 27\n",
            "cost: 11635364.515158571\n",
            "--------------------------\n",
            "iteration: 28\n",
            "cost: 11459201.788556322\n",
            "--------------------------\n",
            "iteration: 29\n",
            "cost: 11292705.26306027\n",
            "--------------------------\n",
            "iteration: 30\n",
            "cost: 11113508.935336266\n",
            "--------------------------\n",
            "iteration: 31\n",
            "cost: 10924514.710093983\n",
            "--------------------------\n",
            "iteration: 32\n",
            "cost: 10708445.642606188\n",
            "--------------------------\n",
            "iteration: 33\n",
            "cost: 10485883.59637861\n",
            "--------------------------\n",
            "iteration: 34\n",
            "cost: 10262985.208500013\n",
            "--------------------------\n",
            "iteration: 35\n",
            "cost: 10037105.544000829\n",
            "--------------------------\n",
            "iteration: 36\n",
            "cost: 9802141.681389803\n",
            "--------------------------\n",
            "iteration: 37\n",
            "cost: 9559814.139775477\n",
            "--------------------------\n",
            "iteration: 38\n",
            "cost: 9289732.856781326\n",
            "--------------------------\n",
            "iteration: 39\n",
            "cost: 9039039.113043642\n",
            "--------------------------\n",
            "iteration: 40\n",
            "cost: 8775525.775792645\n",
            "--------------------------\n",
            "iteration: 41\n",
            "cost: 8489307.412587337\n",
            "--------------------------\n",
            "iteration: 42\n",
            "cost: 8224616.673887913\n",
            "--------------------------\n",
            "iteration: 43\n",
            "cost: 7926368.538536532\n",
            "--------------------------\n",
            "iteration: 44\n",
            "cost: 7660397.511633661\n",
            "--------------------------\n",
            "iteration: 45\n",
            "cost: 7387753.05337674\n",
            "--------------------------\n",
            "iteration: 46\n",
            "cost: 7082135.933863326\n",
            "--------------------------\n",
            "iteration: 47\n",
            "cost: 6777766.367394495\n",
            "--------------------------\n",
            "iteration: 48\n",
            "cost: 6479547.535213713\n",
            "--------------------------\n",
            "iteration: 49\n",
            "cost: 6195303.273273944\n",
            "--------------------------\n",
            "iteration: 50\n",
            "cost: 5893061.127997513\n",
            "--------------------------\n",
            "iteration: 51\n",
            "cost: 5591864.961331614\n",
            "--------------------------\n",
            "iteration: 52\n",
            "cost: 5324226.0659985235\n",
            "--------------------------\n",
            "iteration: 53\n",
            "cost: 5039092.898450192\n",
            "--------------------------\n",
            "iteration: 54\n",
            "cost: 4752959.920074204\n",
            "--------------------------\n",
            "iteration: 55\n",
            "cost: 4494231.141237122\n",
            "--------------------------\n",
            "iteration: 56\n",
            "cost: 4217185.809343431\n",
            "--------------------------\n",
            "iteration: 57\n",
            "cost: 3954035.220533933\n",
            "--------------------------\n",
            "iteration: 58\n",
            "cost: 3701380.101489498\n",
            "--------------------------\n",
            "iteration: 59\n",
            "cost: 3469772.701548826\n",
            "--------------------------\n",
            "iteration: 60\n",
            "cost: 3232016.558887869\n",
            "--------------------------\n",
            "iteration: 61\n",
            "cost: 3008099.161380645\n",
            "--------------------------\n",
            "iteration: 62\n",
            "cost: 2787187.8619497977\n",
            "--------------------------\n",
            "iteration: 63\n",
            "cost: 2595535.6729356\n",
            "--------------------------\n",
            "iteration: 64\n",
            "cost: 2385988.723957106\n",
            "--------------------------\n",
            "iteration: 65\n",
            "cost: 2206281.4107694607\n",
            "--------------------------\n",
            "iteration: 66\n",
            "cost: 2010158.5692752907\n",
            "--------------------------\n",
            "iteration: 67\n",
            "cost: 1846388.7062151919\n",
            "--------------------------\n",
            "iteration: 68\n",
            "cost: 1699621.2173359192\n",
            "--------------------------\n",
            "iteration: 69\n",
            "cost: 1556971.182640174\n",
            "--------------------------\n",
            "iteration: 70\n",
            "cost: 1414753.6019327303\n",
            "--------------------------\n",
            "iteration: 71\n",
            "cost: 1268515.0288895017\n",
            "--------------------------\n",
            "iteration: 72\n",
            "cost: 1140411.6654936054\n",
            "--------------------------\n",
            "iteration: 73\n",
            "cost: 1033768.086806258\n",
            "--------------------------\n",
            "iteration: 74\n",
            "cost: 931236.2852835588\n",
            "--------------------------\n",
            "iteration: 75\n",
            "cost: 823666.6406410129\n",
            "--------------------------\n",
            "iteration: 76\n",
            "cost: 733275.3671086101\n",
            "--------------------------\n",
            "iteration: 77\n",
            "cost: 645082.845438005\n",
            "--------------------------\n",
            "iteration: 78\n",
            "cost: 577086.3088352142\n",
            "--------------------------\n",
            "iteration: 79\n",
            "cost: 515766.4291487518\n",
            "--------------------------\n",
            "iteration: 80\n",
            "cost: 456231.74417703244\n",
            "--------------------------\n",
            "iteration: 81\n",
            "cost: 395450.6534709926\n",
            "--------------------------\n",
            "iteration: 82\n",
            "cost: 338325.6999284673\n",
            "--------------------------\n",
            "iteration: 83\n",
            "cost: 289215.6846675878\n",
            "--------------------------\n",
            "iteration: 84\n",
            "cost: 246757.1705525221\n",
            "--------------------------\n",
            "iteration: 85\n",
            "cost: 218407.72152836184\n",
            "--------------------------\n",
            "iteration: 86\n",
            "cost: 190951.27304693338\n",
            "--------------------------\n",
            "iteration: 87\n",
            "cost: 165173.61721170828\n",
            "--------------------------\n",
            "iteration: 88\n",
            "cost: 141081.35228528592\n",
            "--------------------------\n",
            "iteration: 89\n",
            "cost: 121512.73884723846\n",
            "--------------------------\n",
            "iteration: 90\n",
            "cost: 103087.34293048791\n",
            "--------------------------\n",
            "iteration: 91\n",
            "cost: 88765.90891098026\n",
            "--------------------------\n",
            "iteration: 92\n",
            "cost: 73141.67106273599\n",
            "--------------------------\n",
            "iteration: 93\n",
            "cost: 65571.31285861255\n",
            "--------------------------\n",
            "iteration: 94\n",
            "cost: 57837.85376248758\n",
            "--------------------------\n",
            "iteration: 95\n",
            "cost: 49971.21062583238\n",
            "--------------------------\n",
            "iteration: 96\n",
            "cost: 40260.19235209265\n",
            "--------------------------\n",
            "iteration: 97\n",
            "cost: 33595.760077610204\n",
            "--------------------------\n",
            "iteration: 98\n",
            "cost: 27835.39528099548\n",
            "--------------------------\n",
            "iteration: 99\n",
            "cost: 25844.199622961678\n",
            "run time: 0.23482108116149902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_batch = 0;  rmse_SGD = 0;\n",
        "for i in range(x_test.size):\n",
        "  y_hat_batch = params_hat_batch[0] + params_hat_batch[1] * x_test[i] \n",
        "  y_hat_SGD = params_hat[0] + params_hat[1] * x_test[i] \n",
        "  rmse_batch += (y_hat_batch - y_test[i] )**2\n",
        "  rmse_SGD += (y_test[i] - y_hat_SGD)**2\n",
        "\n",
        "rmse_BatchGradientDescent = np.sqrt(rmse_batch/x_test.size)\n",
        "rmse_SGD = np.sqrt(rmse_SGD/x_test.size)\n",
        "print(\"batch rms:\\t\", rmse_BatchGradientDescent)\n",
        "print(\"stochastic rms:\\t\", rmse_SGD)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6GUOd7AZDT0",
        "outputId": "71697752-5f74-4ca3-ad6a-1cd1281ec893"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch rms:\t 116.92642690633203\n",
            "stochastic rms:\t 156.05006493493576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>cost_batch - cost function computed by batch gradient descent\n",
        "<p>cost - cost function computed by SGD</p>"
      ],
      "metadata": {
        "id": "QD6e-iaR7yx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the code in this cell\n",
        "plt.figure()\n",
        "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
        "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('normalized cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
        "print(f'min cost with SGD: {np.min(cost)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "8OZeQ9TEYzR8",
        "outputId": "6ab302d6-74f0-417f-cce9-7b2ecfc5eaa5"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min cost with BGD: 11835.264951192166\n",
            "min cost with SGD: 25803.971351024677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>Q2.4.  Under this data with 100 datapoints in size, batch gradient descent shows faster reaching minimum cost by same iterations or epoch, but stochastic gradient descent shows faster computation, as show from the above printout (2 second versus 3 second).\n",
        "\n",
        "For small dataset like this example, i think both works pretty similarity with no significant difference.  I would tend to use batch gradient descent under this scenario, in order to get a better minimum cost.\n",
        "\n",
        "However, when dataset is large, stochastic gradient descent would be a better choice, since it doesn't overwhelm or spike computation resources in a single computation, so less memory requirement for real world application."
      ],
      "metadata": {
        "id": "9ga1IVWCejDW"
      }
    }
  ]
}
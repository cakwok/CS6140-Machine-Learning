{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Logistic Regression n Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1oVjd__DS5Qxcxu-LYRfrnmwwQLG84gsQ",
      "authorship_tag": "ABX9TyP6ZBJZ0SrD/9nsArXIUtv3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2270,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LfhZR6MmIFwg",
        "outputId": "d196286d-c943-4087-eb7b-e323961ed1d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCS6140 Assignment 2\\nQuestion 3 - Logistic Regression and Perceptron\\nJun 3 2022\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2270
        }
      ],
      "source": [
        "'''\n",
        "CS6140 Assignment 2\n",
        "Question 3 - Logistic Regression and Perceptron\n",
        "Jun 3 2022\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split    #Library to split training and testing data\n",
        "from sklearn.metrics import classification_report       #Library to compute accuracy, precison and recall"
      ],
      "metadata": {
        "id": "zHyqC_AGJa6E"
      },
      "execution_count": 2271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "def load_data(data_dir):\n",
        "  ''' features_names: input features names\n",
        "      features_data: input features\n",
        "      features_names: output features names\n",
        "      features_labels: output features\n",
        "  '''\n",
        "  dataset = pd.read_csv(data_dir)  \n",
        "\n",
        "  features_names = dataset.columns.values[:-1]        #Equipvalent to feature_cols = ['feature1', 'feature2', 'feature3', 'feature4']\n",
        "  features_data = dataset[features_names] \n",
        "  labels_names = dataset.columns.values[-1]   \n",
        "  labels_data = dataset[labels_name]\n",
        "\n",
        "  return features_names, features_data, labels_names, labels_data"
      ],
      "metadata": {
        "id": "HhgFkJDKJTWc"
      },
      "execution_count": 2272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1 - Implementation of sigmoid function\n",
        "def sigmoid(z):\n",
        "  \"\"\"The sigmoid function.\"\"\"\n",
        "  sigmoid = 1.0/(1.0+np.exp(-z))\n",
        "  sigmoid = np.minimum(sigmoid, 0.9999)  # Set upper bound\n",
        "  sigmoid = np.maximum(sigmoid, 0.0001)  # Set lower bound\n",
        "  return sigmoid"
      ],
      "metadata": {
        "id": "4kWOy-A0SBoR"
      },
      "execution_count": 2273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of cost function\n",
        "def compute_cost(ip, op, params):\n",
        "  \"\"\"\n",
        "  Cost function in linear regression where the cost is calculated\n",
        "  ip: input variables\n",
        "  op: output variables\n",
        "  params: corresponding parameters\n",
        "  nll : negative log likelihood\n",
        "  y_hat : y hat\n",
        "  gradient : gradient\n",
        "  Returns cost, gradient\n",
        "  \"\"\"\n",
        "  \n",
        "  num_of_samples = len(op)\n",
        "\n",
        "  y_hat = sigmoid(np.dot(ip, params))\n",
        "  nll = sum((-op * np.log(y_hat)) - ((1- op)*np.log(1-y_hat)))  \n",
        "  cost = nll\n",
        "  gradient = np.dot(ip.transpose(), (y_hat - op))  \n",
        "\n",
        "  return cost, gradient"
      ],
      "metadata": {
        "id": "ihOBcGt-VRxN"
      },
      "execution_count": 2274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 2 - Implement logistic regression using batch gradient descent and evaluation \n",
        "def logistic_regression_using_batch_gradient_descent(ip, op, params, alpha, num_iter, batch_size = 1):\n",
        "  \"\"\"\n",
        "  Compute the params for logistic regression using batch gradient descent\n",
        "  ip: input variables\n",
        "  op: output variables\n",
        "  params: corresponding parameters\n",
        "  alpha: learning rate\n",
        "  num_iter: number of iterations\n",
        "  cost, cost_list: error function, error function in list format\n",
        "  gradient: gradient\n",
        "  Returns parameters, cost\n",
        "  \"\"\" \n",
        "  # initialize iteration, number of samples, cost and parameter array\n",
        "  cost_list = []\n",
        "  \n",
        "  #batchify the data into mini-batches\n",
        "  pass\n",
        "  \n",
        "  # Compute the cost and store the params for the corresponding cost\n",
        "  for i in range(num_iter):\n",
        "      cost, gradient = compute_cost(ip, op, params)\n",
        "      params = params - (alpha * gradient)\n",
        "      cost_list.append(cost)\n",
        "\n",
        "  return params, cost_list"
      ],
      "metadata": {
        "id": "_MV9qHqY4WDi"
      },
      "execution_count": 2275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation Version 1 : normalize data by z-score\n",
        "def Normalize_zscore(X):\n",
        "  mean = np.mean(X,axis=0)\n",
        "  std = np.std(X,axis=0)\n",
        "  X_norm = (X - mean)/std\n",
        "  \n",
        "  return X_norm "
      ],
      "metadata": {
        "id": "iEJ6_M4O575S"
      },
      "execution_count": 2276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data preparation Version 2 : normalize data by min max normalization\n",
        "def Normalize_minmax(X):\n",
        "  min = np.min(X, axis=0)\n",
        "  max = np.max(X, axis=0)\n",
        "  X_norm = (X - min)/(max - min)\n",
        "  \n",
        "  return X_norm"
      ],
      "metadata": {
        "id": "1Mj5o7tztear"
      },
      "execution_count": 2277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression predictor\n",
        "def Predict(theta,X):\n",
        "    predictions = X.dot(theta)\n",
        "\n",
        "    return np.where(predictions >= 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "GiP19ZGY0j_y"
      },
      "execution_count": 2278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 3 - Implementation of perceptron\n",
        "class Perceptron:\n",
        "  # constructor \n",
        "  def __init__ (self, num_of_x_features, alpha):\n",
        "    #self.weight = np.random.randn(num_of_x_features + 1) / np.sqrt(num_of_x_features)\n",
        "    self.weight = np.zeros((num_of_features+1))\n",
        "    self.b = None\n",
        "    self.alpha = alpha\n",
        "\n",
        "  # model\n",
        "  def classify(self, x):\n",
        "    return 1 if x > 0 else 0\n",
        "\n",
        "  # fitting the model by batch gradient decent\n",
        "  def fit(self, X, Y, epochs): \n",
        "    gradient = 0\n",
        "    for epoch in range(epochs):\n",
        "      for (x, y) in zip(X, Y):\n",
        "        y_hat = self.classify(np.dot(x, self.weight))\n",
        "        if y_hat != y:\n",
        "          gradient += (y - y_hat) * x\n",
        "      self.weight = self.weight + self.alpha * gradient\n",
        "\n",
        "  \"\"\"\n",
        "  # fitting the model by SGD\n",
        "  def fit(self, X, y, epochs): \n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      #gradient = 0\n",
        "      for (x, y) in zip(X, Y):\n",
        "        y_hat = self.classify(np.dot(x, self.weight))\n",
        "        if y_hat != y:\n",
        "          gradient = y - y_hat\n",
        "          self.weight = self.weight + self.alpha * gradient * x\n",
        "  \"\"\"\n",
        " \n",
        "  # predictor to predict on the data based on weight\n",
        "  def predict(self, X):\n",
        "    return self.classify(np.dot(X, self.weight))"
      ],
      "metadata": {
        "id": "2eWmI2EWpnaw"
      },
      "execution_count": 2279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_test, y_predict):\n",
        "  '''return the accuracy scores'''\n",
        "  \n",
        "  #compute accuracy\n",
        "  y_test_array = y_test.to_numpy()    #convert dataframe to numpy array\n",
        "  total_num_samples = y_test.shape[0]\n",
        "\n",
        "  matching_count = np.count_nonzero(y_test_array == y_predict)\n",
        "  accuracy = matching_count/total_num_samples\n",
        "  print(\"matching:\", matching_count, \"total:\", total_num_samples)\n",
        "\n",
        "  #compute precision\n",
        "  true_positive = np.sum((y_test_array == 1) & (y_predict == 1))\n",
        "  false_positive = np.sum((y_test_array == 0) & (y_predict == 1))\n",
        "  false_negative = np.sum((y_test_array == 1) & (y_predict == 0))\n",
        "  precision = true_positive/(true_positive + false_positive)\n",
        "\n",
        "  print(\"true_positive\", true_positive)\n",
        "  print(\"false_positive\", false_positive)\n",
        "  print(\"false_negative\", false_negative)\n",
        "  \n",
        "  #compute recall\n",
        "  recall = true_positive/(true_positive + false_negative)\n",
        "\n",
        "  #put computation result into dataframe\n",
        "  performance_matrix = pd.DataFrame({\"accuracy:\":[accuracy], \"precision\":[precision], \"recall\":[recall], \"total number of samples\":[total_num_samples]})\n",
        "  print(\"\\nperformance\"); print(performance_matrix.to_string(index=False), \"\\n\")\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "M13cb8KJIugk"
      },
      "execution_count": 2280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 4"
      ],
      "metadata": {
        "id": "SJ8cwD4GfQvz"
      },
      "execution_count": 2281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "#reserve the test data, do not use them for cross-validation!\n",
        "\n",
        "#load data\n",
        "DATA_DIR = \"/content/drive/My Drive/Colab Notebooks/CS6140 Assignment2/default of credit card clients2.csv\"\n",
        "features_names, features_data, labels_name, labels_data = load_data(DATA_DIR)\n",
        "\n",
        "#split data\n",
        "x_train, x_test, y_train, y_test = train_test_split(features_data, labels_data, test_size=0.2, random_state = 42) #x_train datatype <class 'pandas.core.frame.DataFrame'>\n",
        "\n",
        "#setup numpy print environment\n",
        "np.set_printoptions(suppress=True)      #with np set to printoptions, the printout would not be in scientific e to the power of x format\n",
        "#np.set_printoptions(threshold=np.inf)"
      ],
      "metadata": {
        "id": "EuLf-likKYtT"
      },
      "execution_count": 2282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#compare results with sklearn logistic regression, raw, logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score \n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "x_train_numpy = x_train.to_numpy()\n",
        "y_train_numpy = y_train.to_numpy()\n",
        "x_test_numpy = x_test.to_numpy()\n",
        "y_test_numpy = y_test.to_numpy()\n",
        "\n",
        "clf = LogisticRegression(class_weight = 'balanced')\n",
        "clf.fit(x_train_numpy, y_train_numpy)\n",
        "y_pred = clf.predict(x_test_numpy)\n",
        "print(\"y_pred\", y_pred)\n",
        "print(accuracy_score(y_pred, y_test_numpy))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "7lkwW4RMTIqv",
        "outputId": "32022276-7776-4fec-be43-9784ee4fef44"
      },
      "execution_count": 2283,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#compare results with sklearn logistic regression, raw, logistic regression\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import confusion_matrix, accuracy_score \\nfrom sklearn.pipeline import make_pipeline\\n\\nx_train_numpy = x_train.to_numpy()\\ny_train_numpy = y_train.to_numpy()\\nx_test_numpy = x_test.to_numpy()\\ny_test_numpy = y_test.to_numpy()\\n\\nclf = LogisticRegression(class_weight = \\'balanced\\')\\nclf.fit(x_train_numpy, y_train_numpy)\\ny_pred = clf.predict(x_test_numpy)\\nprint(\"y_pred\", y_pred)\\nprint(accuracy_score(y_pred, y_test_numpy))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2283
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by logistic regression - normalized data\n",
        "num_of_features = x_train.shape[1]      #x_train.shape (3984, 25)\n",
        "\n",
        "#normalize data\n",
        "x_train_minmax_normalized = Normalize_minmax(x_train)\n",
        "x_test_minmax_normalized = Normalize_minmax(x_test)\n",
        "\n",
        "#add bias column\n",
        "x_train_minmax_normalized = np.append(np.ones((x_train_minmax_normalized.shape[0],1)), x_train_minmax_normalized, axis=1)       #x_train datatype <class 'numpy.ndarray'>, append bias with all ones at the end of dataset\n",
        "x_test_minmax_normalized = np.append(np.ones((x_test_minmax_normalized.shape[0],1)), x_test_minmax_normalized, axis=1)       #x_train datatype <class 'numpy.ndarray'>, append bias with all ones at the end of dataset\n",
        "\n",
        "#reshape true label for calculation\n",
        "y_train_reshaped = y_train.values.reshape(x_train.shape[0],1)\n",
        "y_test_reshaped = y_test.values.reshape(x_test.shape[0],1)\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 200\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))  #initialize params as 0 by rows as num of x_train columns, and 1 single column\n",
        "\n",
        "#train the model - run batch gradient descent\n",
        "params, cost = logistic_regression_using_batch_gradient_descent(x_train_minmax_normalized, y_train_reshaped, params, alpha, num_iters, batch_size)\n",
        "\n",
        "#test the model by prediction\n",
        "#y_normalized_logistic_prediction = []\n",
        "y_predict = Predict(params, x_test_minmax_normalized).transpose()   #has to transpose y_predict for y_test element wise comparison\n",
        "print(\"prediction\", y_predict )\n",
        "print(\"y_test\", y_test.to_numpy())\n",
        "\n",
        "#compute accuracy of the model\n",
        "print(\"manual calculation\")\n",
        "evaluate(y_test, y_predict)\n",
        "print(\"sklearn classification\")\n",
        "print(classification_report(y_test, Predict(params, x_test_minmax_normalized)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih0c9Ux6lF3q",
        "outputId": "d2563f47-4310-4f69-b369-72cec0690bc9"
      },
      "execution_count": 2284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction [[0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0\n",
            "  0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            "  0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            "  0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            "  1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0\n",
            "  0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            "  1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0\n",
            "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            "  1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0]]\n",
            "y_test [1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "manual calculation\n",
            "matching: 771 total: 996\n",
            "true_positive 90\n",
            "false_positive 90\n",
            "false_negative 135\n",
            "\n",
            "performance\n",
            " accuracy:  precision  recall  total number of samples\n",
            "  0.774096        0.5     0.4                      996 \n",
            "\n",
            "sklearn classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       771\n",
            "           1       0.50      0.40      0.44       225\n",
            "\n",
            "    accuracy                           0.77       996\n",
            "   macro avg       0.67      0.64      0.65       996\n",
            "weighted avg       0.76      0.77      0.76       996\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by logistic regression - raw data\n",
        "\n",
        "#add bias column\n",
        "x_test_sklearn_regression = x_test  #to be removed\n",
        "x_train_sklearn_regression = x_train  #to be removed\n",
        "x_test = np.append(np.ones((x_test.shape[0],1)), x_test, axis=1)          #append bias with all ones at the end of dataset\n",
        "x_train = np.append(np.ones((x_train.shape[0],1)), x_train, axis=1)\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 100\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "#train the model - run batch gradient descent\n",
        "params, cost = logistic_regression_using_batch_gradient_descent(x_train, y_train_reshaped, params, alpha, num_iters, batch_size)\n",
        "\n",
        "#test the model by prediction\n",
        "y_predict = Predict(params, x_test).transpose() \n",
        "p = Predict(params, x_test)\n",
        "print(\"prediction\", y_predict)\n",
        "print(\"y_test\", y_test.to_numpy())\n",
        "\n",
        "#compute accuracy of the model\n",
        "evaluate(y_test, y_predict)\n",
        "print(classification_report(y_test,p))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpxZr2bpxVVx",
        "outputId": "11bf143e-5227-418a-fc2a-31f5bb07aafe"
      },
      "execution_count": 2285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction [[1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            "  1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            "  0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            "  1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            "  0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0\n",
            "  1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1\n",
            "  0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0\n",
            "  0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0\n",
            "  0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1\n",
            "  0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            "  1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1\n",
            "  0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
            "  0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0\n",
            "  0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1\n",
            "  1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            "  1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            "  1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1\n",
            "  0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
            "  1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1\n",
            "  1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            "  1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1\n",
            "  1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0\n",
            "  0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1\n",
            "  1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1\n",
            "  1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            "  0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1\n",
            "  0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0]]\n",
            "y_test [1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "matching: 436 total: 996\n",
            "true_positive 147\n",
            "false_positive 482\n",
            "false_negative 78\n",
            "\n",
            "performance\n",
            " accuracy:  precision   recall  total number of samples\n",
            "  0.437751   0.233704 0.653333                      996 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.37      0.51       771\n",
            "           1       0.23      0.65      0.34       225\n",
            "\n",
            "    accuracy                           0.44       996\n",
            "   macro avg       0.51      0.51      0.43       996\n",
            "weighted avg       0.66      0.44      0.47       996\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by perceptron regression - raw data\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 100\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "y_prediction = []\n",
        "\n",
        "#train the model \n",
        "perceptron1 = Perceptron(num_of_features, alpha)\n",
        "perceptron1.fit(x_train, y_train, epochs = num_iters)\n",
        "\n",
        "#test the model\n",
        "for (x, target) in zip(x_test, y_test):\n",
        "  pred = perceptron1.predict(x)\n",
        "  y_prediction.append(pred)\n",
        "\t\n",
        "print(\"y_prediction\",  y_prediction)\n",
        "\n",
        "#compute accuracy of the model\n",
        "evaluate(y_test, y_prediction)\n",
        "print(classification_report(y_test, y_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcRz3cVj6gnl",
        "outputId": "a39ba883-ca48-4761-a547-353197b8957d"
      },
      "execution_count": 2286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_prediction [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "matching: 771 total: 996\n",
            "true_positive 0\n",
            "false_positive 0\n",
            "false_negative 0\n",
            "\n",
            "performance\n",
            " accuracy:  precision  recall  total number of samples\n",
            "  0.774096        NaN     NaN                      996 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      1.00      0.87       771\n",
            "           1       0.00      0.00      0.00       225\n",
            "\n",
            "    accuracy                           0.77       996\n",
            "   macro avg       0.39      0.50      0.44       996\n",
            "weighted avg       0.60      0.77      0.68       996\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict by perceptron regression - normalized data\n",
        "\n",
        "#Initialize parameters\n",
        "alpha=1\n",
        "num_iters = 100\n",
        "batch_size = 1\n",
        "params = np.zeros((num_of_features+1,1))\n",
        "\n",
        "y_normalized_prediction = []\n",
        "\n",
        "#train the model \n",
        "perceptron2 = Perceptron(num_of_features, alpha)\n",
        "perceptron2.fit(x_train_minmax_normalized, y_train, epochs = num_iters)\n",
        "\n",
        "#test the model\n",
        "for (x, target) in zip(x_test_minmax_normalized, y_test):\n",
        "  pred = perceptron2.predict(x)\n",
        "  y_normalized_prediction.append(pred)\n",
        "\n",
        "print(\"y_normalized_prediction\",  y_normalized_prediction)\n",
        "\n",
        "#compute accuracy of the model\n",
        "evaluate(y_test, y_normalized_prediction)\n",
        "print(classification_report(y_test, y_normalized_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqfLMPgeHcGP",
        "outputId": "08b475e0-9c62-4cd7-dae7-e56c6f140ab2"
      },
      "execution_count": 2287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_normalized_prediction [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "matching: 795 total: 996\n",
            "true_positive 0\n",
            "false_positive 0\n",
            "false_negative 0\n",
            "\n",
            "performance\n",
            " accuracy:  precision  recall  total number of samples\n",
            "  0.798193        NaN     NaN                      996 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       771\n",
            "           1       0.60      0.31      0.41       225\n",
            "\n",
            "    accuracy                           0.80       996\n",
            "   macro avg       0.71      0.63      0.64       996\n",
            "weighted avg       0.77      0.80      0.77       996\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ]
    }
  ]
}